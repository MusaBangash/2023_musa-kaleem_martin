{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa6908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693d7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953150f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de554461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir_param = None\n",
    "\n",
    "if parent_dir_param is None:\n",
    "    parent_dir = os.path.abspath(r'../data/')\n",
    "else:\n",
    "    parent_dir = os.path.abspath(parent_dir_param)\n",
    "raw_data_dir = os.path.join(parent_dir, 'raw')\n",
    "processed_data_dir = os.path.join(parent_dir, 'processed_data')\n",
    "concatenated_yaws_dir = os.path.join(parent_dir, 'Concatenated_Yaws')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e75fd",
   "metadata": {},
   "source": [
    "#parent_dir = os.path.abspath(r'D:')\n",
    "\n",
    "#for GitLab\n",
    "#parent_dir = os.path.abspath(r'../data/')\n",
    "\n",
    "# Parameters\n",
    "parent_dir_param = \"{{ parent_dir_param }}\"\n",
    "\n",
    "# Set the parent_dir variable based on the parameter\n",
    "parent_dir = os.path.abspath(parent_dir_param)\n",
    "\n",
    "raw_data_dir = os.path.join(parent_dir, 'raw')\n",
    "processed_data_dir = os.path.join(parent_dir, 'processed_data')\n",
    "concatenated_yaws_dir = os.path.join(parent_dir, 'Concatenated_Yaws')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7d029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc9bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path =raw_data_dir\n",
    "\n",
    "items = os.listdir(dir_path)\n",
    "\n",
    "# Filter out only the directories\n",
    "folders = [item for item in items if os.path.isdir(os.path.join(dir_path, item))]\n",
    "\n",
    "for folder_name in folders:\n",
    "    folder_path = os.path.join(dir_path, folder_name)\n",
    "    os.chdir(folder_path)\n",
    "    all_files = glob.glob('*int')\n",
    "    print(f\"Processing {len(all_files)} files in folder: {folder_name}\")\n",
    "    print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd5b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07cb4ffc",
   "metadata": {},
   "source": [
    "## Read Binary (read_int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5754005",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = (1,4,5,6,7,8,9,10,11,12,53,54,56,57,59,60,61,62,63,64)\n",
    "\n",
    "\n",
    "\n",
    "def read_int(FileName,x):\n",
    "\n",
    "    #print(FileName)\n",
    "\n",
    "    xlen = len(FileName)\n",
    "    ending = FileName[xlen-4:xlen]\n",
    "\n",
    "    if ending=='.int':\n",
    "        FileName = FileName[0:xlen-4]\n",
    "\n",
    "\n",
    "    with open(FileName+ending,\"rb\") as f:\n",
    "        f.read()\n",
    "        filesize = f.tell()\n",
    "       # print(filesize)\n",
    "       \n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with open(FileName+ending,\"rb\") as f:\n",
    "\n",
    "    # the first 4 junk buffer\n",
    "        \n",
    "        leer = np.frombuffer(f.read(4),dtype=np.single)\n",
    "        #print(leer)\n",
    "    # the  24 buffer give information about file\n",
    "        datei = np.frombuffer(f.read(24),dtype=np.uint32)\n",
    "        #print(datei)\n",
    "\n",
    "        f.seek(76)\n",
    "    # read the number of sensor in file \n",
    "        lvek = np.frombuffer(f.read(4),dtype=np.uint32)\n",
    "        #print(lvek)\n",
    "     \n",
    "    # then 4 times the number of sensors are skipped to read a control parameter:\n",
    "  \n",
    "        f.seek(84+lvek[0]*4)\n",
    "\n",
    "        #filetype = np.frombuffer(f.read(4),dtype=np.uint)\n",
    "        #print(filetype)\n",
    "        \n",
    "        filetype = np.frombuffer(f.read(4), dtype=np.uint32)\n",
    "\n",
    "\n",
    "        if filetype != 12:\n",
    "\n",
    "    # start time is read (is not equal to 0 as we have been cut off from the simulation for the first 50 s\n",
    "            t0 = np.frombuffer(f.read(4),dtype=np.single)\n",
    "            #print(t0)\n",
    "\n",
    "    # comes the parameter for the step size of the simulation\n",
    "\n",
    "            dt = np.frombuffer(f.read(4),dtype=np.single)\n",
    "            #print(dt)\n",
    "\n",
    "            pos_fak = f.tell()\n",
    "            #print(pos_fak)\n",
    "\n",
    "            fak = []\n",
    " \n",
    "            for n in x:\n",
    "                f.seek(pos_fak + 4*(n-1))\n",
    "                fak.append(np.frombuffer(f.read(4),dtype=np.single))\n",
    "                \n",
    "            f.seek(pos_fak+lvek[0]*4)\n",
    "            position = f.tell()\n",
    "    # the length of the sensors is calculated:\n",
    "            RecordCount = round((filesize-position)/lvek[0]/2)\n",
    "            #print(RecordCount)\n",
    "\n",
    "            raw_ts_int = np.empty([len(x),RecordCount])\n",
    "           # print(raw_ts_int)\n",
    "        \n",
    "            #print(\"Type:\",type(raw_ts_int))\n",
    "            #print(raw_ts_int.shape)\n",
    "\n",
    "            for n in range(RecordCount):\n",
    "\n",
    "                for m, sensor_ in enumerate(x):\n",
    "\n",
    "                    f.seek(position+2*sensor_+(lvek[0]*2)*n-2)\n",
    "                    raw_ts_int[m,n] = np.frombuffer(f.read(2),dtype=np.int16)*fak[m]\n",
    "\n",
    "            global   df\n",
    "            df = pd.DataFrame(raw_ts_int)\n",
    "\n",
    "            #print(df.transpose())\n",
    "            #merged_df=df.merge(df)\n",
    "                    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ecf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d8621a1",
   "metadata": {},
   "source": [
    "## Extracted Statistical Data and Assigning the Column Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c977ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sensor = (1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 53, 54, 56, 57, 59, 60, 61, 62, 63, 64)\n",
    "\n",
    "data_dir = raw_data_dir\n",
    "output_dir = processed_data_dir\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for folder_name in dirs:\n",
    "        print(f\"Processing data in {folder_name}...\")\n",
    "        \n",
    "        folder_path = os.path.join(root, folder_name)\n",
    "        output_folder_path = os.path.join(output_dir, folder_name)\n",
    "        os.makedirs(output_folder_path, exist_ok=True)\n",
    "        \n",
    "        os.chdir(folder_path)\n",
    "        \n",
    "        file_list = glob.glob(\"*.int\")\n",
    "        \n",
    "        mean_df = pd.DataFrame()\n",
    "        min_df = pd.DataFrame()\n",
    "        max_df = pd.DataFrame()\n",
    "        median_df = pd.DataFrame()\n",
    "        std_df = pd.DataFrame()\n",
    "        q1_df = pd.DataFrame()\n",
    "        q3_df = pd.DataFrame()\n",
    "        \n",
    "        for file_path in file_list:\n",
    "            read_int(file_path, sensor) \n",
    "            data = df.transpose()\n",
    "    \n",
    "            mean_df = pd.concat([mean_df, data.mean()], axis=1)\n",
    "            min_df = pd.concat([min_df, data.min()], axis=1)\n",
    "            max_df = pd.concat([max_df, data.max()], axis=1)\n",
    "            median_df = pd.concat([median_df, data.median()], axis=1)\n",
    "            std_df = pd.concat([std_df, data.std()], axis=1)\n",
    "            q1_df = pd.concat([q1_df, data.quantile(0.25)], axis=1)\n",
    "            q3_df = pd.concat([q3_df, data.quantile(0.75)], axis=1)\n",
    "    \n",
    "        mean_df = mean_df.transpose().reset_index(drop=True)\n",
    "        min_df = min_df.transpose().reset_index(drop=True)\n",
    "        max_df = max_df.transpose().reset_index(drop=True)\n",
    "        median_df = median_df.transpose().reset_index(drop=True)\n",
    "        std_df = std_df.transpose().reset_index(drop=True)\n",
    "        q1_df = q1_df.transpose().reset_index(drop=True)\n",
    "        q3_df = q3_df.transpose().reset_index(drop=True)\n",
    "        \n",
    "        mean_df.columns = ['wind_speed', 'yaw_error', 'pitch_angle_b1', 'pitch_angle_b2', 'pitch_angle_b3', 'rotorposition', 'generator_speed', 'rotor_speed', 'generator_torque', 'power', 'acceleration_CROSS', 'acceleration_THRUST', 'tower_deflection_Y', 'tower_deflection_Z', 'tip_deflection_flap_V1', 'tip_deflection_flap_V2', 'tip_deflection_flap_V3', 'tip_deflection_edge_V1', 'tip_deflection_edge_V2', 'tip_deflection_edge_V3']\n",
    "        min_df.columns = mean_df.columns\n",
    "        max_df.columns = mean_df.columns\n",
    "        median_df.columns = mean_df.columns\n",
    "        std_df.columns = mean_df.columns\n",
    "        q1_df.columns = mean_df.columns\n",
    "        q3_df.columns = mean_df.columns\n",
    "    \n",
    "        mean_df.to_csv(os.path.join(output_folder_path, f\"{folder_name}_mean.csv\"), index=False)\n",
    "        min_df.to_csv(os.path.join(output_folder_path, f\"{folder_name}_min.csv\"), index=False)\n",
    "        max_df.to_csv(os.path.join(output_folder_path, f\"{folder_name}_max.csv\"), index=False)\n",
    "        median_df.to_csv(os.path.join(output_folder_path, f\"{folder_name}_median.csv\"), index=False)\n",
    "        std_df.to_csv(os.path.join(output_folder_path, f\"{folder_name}_std.csv\"), index=False)\n",
    "        q1_df.to_csv(os.path.join(output_folder_path, f\"{folder_name}_q1.csv\"), index=False)\n",
    "        q3_df.to_csv(os.path.join(output_folder_path, f\"{folder_name}_q3.csv\"), index=False)\n",
    "\n",
    "print(\"Processing done for all folders.\")\n",
    "\n",
    "column_names = {\n",
    "    '0': 'wind_speed',\n",
    "    '1': 'yaw_error',\n",
    "    '2': 'pitch_angle_b1',\n",
    "    '3': 'pitch_angle_b2',\n",
    "    '4': 'pitch_angle_b3',\n",
    "    '5': 'rotorposition',\n",
    "    '6': 'generator_speed',\n",
    "    '7': 'rotor_speed',\n",
    "    '8': 'generator_torque',\n",
    "    '9': 'power',\n",
    "    '10': 'acceleration_CROSS',\n",
    "    '11': 'acceleration_THRUST',\n",
    "    '12': 'tower_deflection_Y',\n",
    "    '13': 'tower_deflection_Z',\n",
    "    '14': 'tip_deflection_flap_V1',\n",
    "    '15': 'tip_deflection_flap_V2',\n",
    "    '16': 'tip_deflection_flap_V3',\n",
    "    '17': 'tip_deflection_edge_V1',\n",
    "    '18': 'tip_deflection_edge_V2',\n",
    "    '19': 'tip_deflection_edge_V3',\n",
    "}\n",
    "\n",
    "for folder_name in os.listdir(output_dir):\n",
    "    folder_path = os.path.join(output_dir, folder_name)\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    for file_name in csv_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df.rename(columns=column_names, inplace=True)\n",
    "\n",
    "        new_file_name = os.path.splitext(file_name)[0]\n",
    "        new_file_path = os.path.join(folder_path, new_file_name + '.parquet')\n",
    "\n",
    "        df.to_parquet(new_file_path, index=False)\n",
    "\n",
    "print(\"Column names changed and files saved as Parquet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358f408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49ca6590",
   "metadata": {},
   "source": [
    "## Exporting extracted data to required directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eac068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def concatenate_csv_files(directory):\n",
    "    os.chdir(directory)\n",
    "\n",
    "    csv_files = [file for file in os.listdir() if file.endswith('.csv')]\n",
    "\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "    for file in csv_files:\n",
    "        aggregation_type = file.split('.')[0].split('_')[-1]\n",
    "        df = pd.read_csv(file)\n",
    "        renamed_columns = [f\"{column}_{aggregation_type}\" for column in df.columns]\n",
    "        df.columns = renamed_columns\n",
    "        combined_data = pd.concat([combined_data, df], axis=1)\n",
    "\n",
    "    sorted_columns = sorted(combined_data.columns)\n",
    "    combined_data = combined_data[sorted_columns]\n",
    "\n",
    "    directory_name = os.path.basename(directory)\n",
    "\n",
    "    output_file_name = f'{directory_name}_combined_data.csv'\n",
    "    output_directory = os.path.join(processed_data_dir, 'Concatenated_Yaws')\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_file_path = os.path.join(output_directory, output_file_name)\n",
    "    combined_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"The data from {len(csv_files)} CSV files in directory {directory} has been concatenated, rearranged, and exported to {output_file_path}.\")\n",
    "\n",
    "\n",
    "base_directory = processed_data_dir\n",
    "\n",
    "yaw_directories = [os.path.join(base_directory, directory) for directory in os.listdir(base_directory) if directory.startswith('Yaw')]\n",
    "\n",
    "for directory in yaw_directories:\n",
    "    concatenate_csv_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ebb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7e564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59057f76",
   "metadata": {},
   "source": [
    " import pandas as pd\n",
    "import osdef concatenate_csv_files(directory):\n",
    "    os.chdir(directory)csv_files = [file for file in os.listdir() if file.endswith('.csv')]\n",
    "\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "for file in csv_files:\n",
    "    aggregation_type = file.split('.')[0].split('_')[-1]\n",
    "    df = pd.read_csv(file)\n",
    "    renamed_columns = [f\"{column}_{aggregation_type}\" for column in df.columns]\n",
    "    df.columns = renamed_columns\n",
    "    combined_data = pd.concat([combined_data, df], axis=1)\n",
    "\n",
    "sorted_columns = sorted(combined_data.columns)\n",
    "combined_data = combined_data[sorted_columns]\n",
    "\n",
    "directory_name = os.path.basename(directory)\n",
    "\n",
    "output_file_name = f'{directory_name}_combined_data.csv'\n",
    "output_directory = concate_yaw_dir\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_file_path = os.path.join(output_directory, output_file_name)\n",
    "combined_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"The data from {len(csv_files)} CSV files in directory {directory} has been concatenated, rearranged, and exported to {output_file_path}.\")\n",
    "\n",
    "base_directory = processed_data_diryaw_directories = [os.path.join(base_directory, directory) for directory in os.listdir(base_directory) if directory.startswith('Yaw')]for directory in yaw_directories:\n",
    "    concatenate_csv_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d72d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf563dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e980bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b506cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d1844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d1ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
