{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e33e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import plotly.express as px\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a840f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d58bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(r'D:')\n",
    "output_dir = os.path.join(parent_dir, 'processed_data', 'ModelingData')\n",
    "\n",
    "big_train_path = os.path.join(output_dir, 'big_train.parquet')\n",
    "big_train_df = pd.read_parquet(big_train_path)\n",
    "\n",
    "big_test_path = os.path.join(output_dir, 'big_test.parquet')\n",
    "big_test_df = pd.read_parquet(big_test_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef79ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01dff3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'wind_speed_mean', 'Turbulence',\n",
    "    'tip_deflection_edge_V1_mean', 'tip_deflection_edge_V2_mean',\n",
    "    'tip_deflection_edge_V3_mean', 'tip_deflection_flap_V1_mean',\n",
    "    'tip_deflection_flap_V2_mean', 'tip_deflection_flap_V3_mean',\n",
    "    'tower_deflection_Y_mean', 'tower_deflection_Z_mean',\n",
    "    'wind_speed_max', 'wind_speed_median', 'wind_speed_min',\n",
    "    'wind_speed_q1', 'wind_speed_q3', 'wind_speed_std',\n",
    "    'yaw_error_max', 'yaw_error_median', 'yaw_error_min',\n",
    "    'yaw_error_q1', 'yaw_error_q3', 'yaw_error_std',\n",
    "    'tip_deflection_edge_V1_max', 'tip_deflection_edge_V1_median',\n",
    "    'tip_deflection_edge_V1_min', 'tip_deflection_edge_V1_q1',\n",
    "    'tip_deflection_edge_V1_q3', 'tip_deflection_edge_V1_std',\n",
    "    'tip_deflection_edge_V2_max', 'tip_deflection_edge_V2_median',\n",
    "    'tip_deflection_edge_V2_min', 'tip_deflection_edge_V2_q1',\n",
    "    'tip_deflection_edge_V2_q3', 'tip_deflection_edge_V2_std',\n",
    "    'tip_deflection_edge_V3_max', 'tip_deflection_edge_V3_median',\n",
    "    'tip_deflection_edge_V3_min', 'tip_deflection_edge_V3_q1',\n",
    "    'tip_deflection_edge_V3_q3', 'tip_deflection_edge_V3_std',\n",
    "    'tip_deflection_flap_V1_max', 'tip_deflection_flap_V1_median',\n",
    "    'tip_deflection_flap_V1_min', 'tip_deflection_flap_V1_q1',\n",
    "    'tip_deflection_flap_V1_q3', 'tip_deflection_flap_V1_std',\n",
    "    'tip_deflection_flap_V2_max', 'tip_deflection_flap_V2_median',\n",
    "    'tip_deflection_flap_V2_min', 'tip_deflection_flap_V2_q1',\n",
    "    'tip_deflection_flap_V2_q3', 'tip_deflection_flap_V2_std',\n",
    "    'tip_deflection_flap_V3_max', 'tip_deflection_flap_V3_median',\n",
    "    'tip_deflection_flap_V3_min', 'tip_deflection_flap_V3_q1',\n",
    "    'tip_deflection_flap_V3_q3', 'tip_deflection_flap_V3_std',\n",
    "    'tower_deflection_Y_max', 'tower_deflection_Y_median',\n",
    "    'tower_deflection_Y_min', 'tower_deflection_Y_q1',\n",
    "    'tower_deflection_Y_q3', 'tower_deflection_Y_std',\n",
    "    'tower_deflection_Z_max', 'tower_deflection_Z_median',\n",
    "    'tower_deflection_Z_min', 'tower_deflection_Z_q1',\n",
    "    'tower_deflection_Z_q3', 'tower_deflection_Z_std'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d97fb451",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_train_updated = big_train_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "big_test_updated = big_test_df.drop(columns_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36f93e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceleration_CROSS_max</th>\n",
       "      <th>acceleration_CROSS_mean</th>\n",
       "      <th>acceleration_CROSS_median</th>\n",
       "      <th>acceleration_CROSS_min</th>\n",
       "      <th>acceleration_CROSS_q1</th>\n",
       "      <th>acceleration_CROSS_q3</th>\n",
       "      <th>acceleration_CROSS_std</th>\n",
       "      <th>acceleration_THRUST_max</th>\n",
       "      <th>acceleration_THRUST_mean</th>\n",
       "      <th>acceleration_THRUST_median</th>\n",
       "      <th>...</th>\n",
       "      <th>rotor_speed_q3</th>\n",
       "      <th>rotor_speed_std</th>\n",
       "      <th>rotorposition_max</th>\n",
       "      <th>rotorposition_mean</th>\n",
       "      <th>rotorposition_median</th>\n",
       "      <th>rotorposition_min</th>\n",
       "      <th>rotorposition_q1</th>\n",
       "      <th>rotorposition_q3</th>\n",
       "      <th>rotorposition_std</th>\n",
       "      <th>yaw_error_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>0.601625</td>\n",
       "      <td>1.403746e-04</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>-0.523733</td>\n",
       "      <td>-0.106093</td>\n",
       "      <td>0.109458</td>\n",
       "      <td>0.158098</td>\n",
       "      <td>1.220294</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>...</td>\n",
       "      <td>14.650212</td>\n",
       "      <td>0.318464</td>\n",
       "      <td>359.994568</td>\n",
       "      <td>179.722212</td>\n",
       "      <td>179.367294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.694893</td>\n",
       "      <td>269.635925</td>\n",
       "      <td>103.843829</td>\n",
       "      <td>-1.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.296685</td>\n",
       "      <td>5.274648e-05</td>\n",
       "      <td>-0.000524</td>\n",
       "      <td>-0.295683</td>\n",
       "      <td>-0.051593</td>\n",
       "      <td>0.052733</td>\n",
       "      <td>0.079899</td>\n",
       "      <td>0.672260</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>-0.001873</td>\n",
       "      <td>...</td>\n",
       "      <td>14.605525</td>\n",
       "      <td>0.228010</td>\n",
       "      <td>359.996979</td>\n",
       "      <td>179.872840</td>\n",
       "      <td>179.773483</td>\n",
       "      <td>0.056250</td>\n",
       "      <td>90.134239</td>\n",
       "      <td>269.648987</td>\n",
       "      <td>103.817809</td>\n",
       "      <td>-0.817816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>0.080912</td>\n",
       "      <td>-6.143227e-06</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.093744</td>\n",
       "      <td>-0.020088</td>\n",
       "      <td>0.020431</td>\n",
       "      <td>0.027331</td>\n",
       "      <td>0.196996</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.000323</td>\n",
       "      <td>...</td>\n",
       "      <td>9.891097</td>\n",
       "      <td>0.906677</td>\n",
       "      <td>359.999420</td>\n",
       "      <td>179.929879</td>\n",
       "      <td>179.875961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.471107</td>\n",
       "      <td>270.314575</td>\n",
       "      <td>104.132610</td>\n",
       "      <td>-1.229728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>0.106859</td>\n",
       "      <td>-1.343214e-05</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-0.094594</td>\n",
       "      <td>-0.020894</td>\n",
       "      <td>0.021028</td>\n",
       "      <td>0.029720</td>\n",
       "      <td>0.224062</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000607</td>\n",
       "      <td>...</td>\n",
       "      <td>8.822126</td>\n",
       "      <td>0.427362</td>\n",
       "      <td>359.995728</td>\n",
       "      <td>180.107981</td>\n",
       "      <td>180.357864</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>89.920181</td>\n",
       "      <td>270.323029</td>\n",
       "      <td>104.011773</td>\n",
       "      <td>-0.312679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>0.203900</td>\n",
       "      <td>5.211188e-05</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>-0.202273</td>\n",
       "      <td>-0.033194</td>\n",
       "      <td>0.033619</td>\n",
       "      <td>0.049784</td>\n",
       "      <td>0.424746</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>...</td>\n",
       "      <td>14.316802</td>\n",
       "      <td>0.598483</td>\n",
       "      <td>359.998871</td>\n",
       "      <td>180.116412</td>\n",
       "      <td>180.269440</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>89.920967</td>\n",
       "      <td>270.190399</td>\n",
       "      <td>104.014599</td>\n",
       "      <td>-0.709195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>0.786661</td>\n",
       "      <td>1.809045e-04</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>-0.755932</td>\n",
       "      <td>-0.152489</td>\n",
       "      <td>0.149736</td>\n",
       "      <td>0.224245</td>\n",
       "      <td>1.071703</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>-0.001298</td>\n",
       "      <td>...</td>\n",
       "      <td>14.643973</td>\n",
       "      <td>0.293615</td>\n",
       "      <td>359.971832</td>\n",
       "      <td>179.741964</td>\n",
       "      <td>179.524704</td>\n",
       "      <td>0.022498</td>\n",
       "      <td>89.891716</td>\n",
       "      <td>269.562653</td>\n",
       "      <td>103.830126</td>\n",
       "      <td>5.034394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>0.077625</td>\n",
       "      <td>-4.414537e-06</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.077526</td>\n",
       "      <td>-0.010631</td>\n",
       "      <td>0.010867</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>0.178080</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.744234</td>\n",
       "      <td>0.103144</td>\n",
       "      <td>359.987457</td>\n",
       "      <td>180.054447</td>\n",
       "      <td>180.072479</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>90.086861</td>\n",
       "      <td>270.058105</td>\n",
       "      <td>103.911625</td>\n",
       "      <td>4.851429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>0.859016</td>\n",
       "      <td>-2.521636e-04</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>-0.835823</td>\n",
       "      <td>-0.157200</td>\n",
       "      <td>0.155294</td>\n",
       "      <td>0.227695</td>\n",
       "      <td>1.489572</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.005494</td>\n",
       "      <td>...</td>\n",
       "      <td>14.678144</td>\n",
       "      <td>0.348346</td>\n",
       "      <td>359.974121</td>\n",
       "      <td>179.874500</td>\n",
       "      <td>179.773331</td>\n",
       "      <td>0.011249</td>\n",
       "      <td>89.914787</td>\n",
       "      <td>269.856842</td>\n",
       "      <td>103.902284</td>\n",
       "      <td>4.908555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>0.140817</td>\n",
       "      <td>8.278696e-07</td>\n",
       "      <td>-0.000521</td>\n",
       "      <td>-0.128994</td>\n",
       "      <td>-0.023376</td>\n",
       "      <td>0.023724</td>\n",
       "      <td>0.033953</td>\n",
       "      <td>0.249307</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>...</td>\n",
       "      <td>14.359143</td>\n",
       "      <td>0.134628</td>\n",
       "      <td>359.959259</td>\n",
       "      <td>179.928116</td>\n",
       "      <td>179.732147</td>\n",
       "      <td>0.067492</td>\n",
       "      <td>89.821083</td>\n",
       "      <td>270.273163</td>\n",
       "      <td>104.010188</td>\n",
       "      <td>4.953826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>0.325140</td>\n",
       "      <td>3.080633e-06</td>\n",
       "      <td>-0.000711</td>\n",
       "      <td>-0.386240</td>\n",
       "      <td>-0.069844</td>\n",
       "      <td>0.069809</td>\n",
       "      <td>0.098892</td>\n",
       "      <td>0.791757</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>...</td>\n",
       "      <td>14.635700</td>\n",
       "      <td>0.265532</td>\n",
       "      <td>359.998077</td>\n",
       "      <td>179.649847</td>\n",
       "      <td>179.391541</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>89.707024</td>\n",
       "      <td>269.559814</td>\n",
       "      <td>103.865180</td>\n",
       "      <td>4.976628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62074 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      acceleration_CROSS_max  acceleration_CROSS_mean  \\\n",
       "892                 0.601625             1.403746e-04   \n",
       "610                 0.296685             5.274648e-05   \n",
       "1470                0.080912            -6.143227e-06   \n",
       "1632                0.106859            -1.343214e-05   \n",
       "584                 0.203900             5.211188e-05   \n",
       "...                      ...                      ...   \n",
       "546                 0.786661             1.809045e-04   \n",
       "658                 0.077625            -4.414537e-06   \n",
       "1542                0.859016            -2.521636e-04   \n",
       "1019                0.140817             8.278696e-07   \n",
       "1578                0.325140             3.080633e-06   \n",
       "\n",
       "      acceleration_CROSS_median  acceleration_CROSS_min  \\\n",
       "892                    0.001354               -0.523733   \n",
       "610                   -0.000524               -0.295683   \n",
       "1470                  -0.000057               -0.093744   \n",
       "1632                   0.000067               -0.094594   \n",
       "584                    0.000349               -0.202273   \n",
       "...                         ...                     ...   \n",
       "546                    0.001401               -0.755932   \n",
       "658                   -0.000148               -0.077526   \n",
       "1542                   0.001208               -0.835823   \n",
       "1019                  -0.000521               -0.128994   \n",
       "1578                  -0.000711               -0.386240   \n",
       "\n",
       "      acceleration_CROSS_q1  acceleration_CROSS_q3  acceleration_CROSS_std  \\\n",
       "892               -0.106093               0.109458                0.158098   \n",
       "610               -0.051593               0.052733                0.079899   \n",
       "1470              -0.020088               0.020431                0.027331   \n",
       "1632              -0.020894               0.021028                0.029720   \n",
       "584               -0.033194               0.033619                0.049784   \n",
       "...                     ...                    ...                     ...   \n",
       "546               -0.152489               0.149736                0.224245   \n",
       "658               -0.010631               0.010867                0.019850   \n",
       "1542              -0.157200               0.155294                0.227695   \n",
       "1019              -0.023376               0.023724                0.033953   \n",
       "1578              -0.069844               0.069809                0.098892   \n",
       "\n",
       "      acceleration_THRUST_max  acceleration_THRUST_mean  \\\n",
       "892                  1.220294                 -0.000128   \n",
       "610                  0.672260                  0.000096   \n",
       "1470                 0.196996                  0.000046   \n",
       "1632                 0.224062                  0.000026   \n",
       "584                  0.424746                  0.000063   \n",
       "...                       ...                       ...   \n",
       "546                  1.071703                  0.000401   \n",
       "658                  0.178080                  0.000023   \n",
       "1542                 1.489572                  0.000051   \n",
       "1019                 0.249307                  0.000054   \n",
       "1578                 0.791757                  0.000393   \n",
       "\n",
       "      acceleration_THRUST_median  ...  rotor_speed_q3  rotor_speed_std  \\\n",
       "892                     0.000514  ...       14.650212         0.318464   \n",
       "610                    -0.001873  ...       14.605525         0.228010   \n",
       "1470                   -0.000323  ...        9.891097         0.906677   \n",
       "1632                   -0.000607  ...        8.822126         0.427362   \n",
       "584                     0.000360  ...       14.316802         0.598483   \n",
       "...                          ...  ...             ...              ...   \n",
       "546                    -0.001298  ...       14.643973         0.293615   \n",
       "658                     0.000000  ...        7.744234         0.103144   \n",
       "1542                    0.005494  ...       14.678144         0.348346   \n",
       "1019                    0.000626  ...       14.359143         0.134628   \n",
       "1578                    0.001060  ...       14.635700         0.265532   \n",
       "\n",
       "      rotorposition_max  rotorposition_mean  rotorposition_median  \\\n",
       "892          359.994568          179.722212            179.367294   \n",
       "610          359.996979          179.872840            179.773483   \n",
       "1470         359.999420          179.929879            179.875961   \n",
       "1632         359.995728          180.107981            180.357864   \n",
       "584          359.998871          180.116412            180.269440   \n",
       "...                 ...                 ...                   ...   \n",
       "546          359.971832          179.741964            179.524704   \n",
       "658          359.987457          180.054447            180.072479   \n",
       "1542         359.974121          179.874500            179.773331   \n",
       "1019         359.959259          179.928116            179.732147   \n",
       "1578         359.998077          179.649847            179.391541   \n",
       "\n",
       "      rotorposition_min  rotorposition_q1  rotorposition_q3  \\\n",
       "892            0.000000         89.694893        269.635925   \n",
       "610            0.056250         90.134239        269.648987   \n",
       "1470           0.000000         89.471107        270.314575   \n",
       "1632           0.011250         89.920181        270.323029   \n",
       "584            0.011250         89.920967        270.190399   \n",
       "...                 ...               ...               ...   \n",
       "546            0.022498         89.891716        269.562653   \n",
       "658            0.011250         90.086861        270.058105   \n",
       "1542           0.011249         89.914787        269.856842   \n",
       "1019           0.067492         89.821083        270.273163   \n",
       "1578           0.022500         89.707024        269.559814   \n",
       "\n",
       "      rotorposition_std  yaw_error_mean  \n",
       "892          103.843829       -1.115500  \n",
       "610          103.817809       -0.817816  \n",
       "1470         104.132610       -1.229728  \n",
       "1632         104.011773       -0.312679  \n",
       "584          104.014599       -0.709195  \n",
       "...                 ...             ...  \n",
       "546          103.830126        5.034394  \n",
       "658          103.911625        4.851429  \n",
       "1542         103.902284        4.908555  \n",
       "1019         104.010188        4.953826  \n",
       "1578         103.865180        4.976628  \n",
       "\n",
       "[62074 rows x 71 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_train_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6affe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceleration_CROSS_max</th>\n",
       "      <th>acceleration_CROSS_mean</th>\n",
       "      <th>acceleration_CROSS_median</th>\n",
       "      <th>acceleration_CROSS_min</th>\n",
       "      <th>acceleration_CROSS_q1</th>\n",
       "      <th>acceleration_CROSS_q3</th>\n",
       "      <th>acceleration_CROSS_std</th>\n",
       "      <th>acceleration_THRUST_max</th>\n",
       "      <th>acceleration_THRUST_mean</th>\n",
       "      <th>acceleration_THRUST_median</th>\n",
       "      <th>...</th>\n",
       "      <th>rotor_speed_q3</th>\n",
       "      <th>rotor_speed_std</th>\n",
       "      <th>rotorposition_max</th>\n",
       "      <th>rotorposition_mean</th>\n",
       "      <th>rotorposition_median</th>\n",
       "      <th>rotorposition_min</th>\n",
       "      <th>rotorposition_q1</th>\n",
       "      <th>rotorposition_q3</th>\n",
       "      <th>rotorposition_std</th>\n",
       "      <th>yaw_error_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.079123</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>-0.081799</td>\n",
       "      <td>-0.006320</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.016439</td>\n",
       "      <td>0.098308</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>...</td>\n",
       "      <td>7.713003</td>\n",
       "      <td>0.059454</td>\n",
       "      <td>359.996094</td>\n",
       "      <td>179.771935</td>\n",
       "      <td>179.570541</td>\n",
       "      <td>0.033750</td>\n",
       "      <td>89.852768</td>\n",
       "      <td>269.434570</td>\n",
       "      <td>103.909795</td>\n",
       "      <td>-1.220241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.080641</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>-0.079200</td>\n",
       "      <td>-0.010629</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>0.017438</td>\n",
       "      <td>0.162419</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>...</td>\n",
       "      <td>10.392641</td>\n",
       "      <td>0.360333</td>\n",
       "      <td>359.993561</td>\n",
       "      <td>180.052960</td>\n",
       "      <td>180.064270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.110886</td>\n",
       "      <td>270.040161</td>\n",
       "      <td>103.919746</td>\n",
       "      <td>-1.081554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.114809</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>-0.132331</td>\n",
       "      <td>-0.018883</td>\n",
       "      <td>0.018894</td>\n",
       "      <td>0.028989</td>\n",
       "      <td>0.204867</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>13.281627</td>\n",
       "      <td>0.518050</td>\n",
       "      <td>359.995972</td>\n",
       "      <td>180.233785</td>\n",
       "      <td>180.515472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.347740</td>\n",
       "      <td>270.210724</td>\n",
       "      <td>103.893912</td>\n",
       "      <td>-0.955399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.143640</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>-0.130101</td>\n",
       "      <td>-0.022333</td>\n",
       "      <td>0.022096</td>\n",
       "      <td>0.033276</td>\n",
       "      <td>0.293878</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>...</td>\n",
       "      <td>13.899364</td>\n",
       "      <td>0.368430</td>\n",
       "      <td>359.996155</td>\n",
       "      <td>180.218371</td>\n",
       "      <td>180.481812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.269035</td>\n",
       "      <td>270.109589</td>\n",
       "      <td>103.856943</td>\n",
       "      <td>-0.905213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.170798</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>-0.176361</td>\n",
       "      <td>-0.027387</td>\n",
       "      <td>0.027084</td>\n",
       "      <td>0.043117</td>\n",
       "      <td>0.314227</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000444</td>\n",
       "      <td>...</td>\n",
       "      <td>14.411539</td>\n",
       "      <td>0.189976</td>\n",
       "      <td>359.998138</td>\n",
       "      <td>179.879823</td>\n",
       "      <td>179.830322</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>89.943283</td>\n",
       "      <td>269.852356</td>\n",
       "      <td>103.917794</td>\n",
       "      <td>-1.059410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>0.504826</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>-0.540348</td>\n",
       "      <td>-0.127715</td>\n",
       "      <td>0.131784</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>1.355515</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>...</td>\n",
       "      <td>14.681202</td>\n",
       "      <td>0.343336</td>\n",
       "      <td>359.985352</td>\n",
       "      <td>179.604842</td>\n",
       "      <td>179.351456</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>89.602600</td>\n",
       "      <td>269.572784</td>\n",
       "      <td>103.891620</td>\n",
       "      <td>4.968577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>0.523959</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.599360</td>\n",
       "      <td>-0.134737</td>\n",
       "      <td>0.138828</td>\n",
       "      <td>0.185669</td>\n",
       "      <td>1.356576</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.004309</td>\n",
       "      <td>...</td>\n",
       "      <td>14.682295</td>\n",
       "      <td>0.347207</td>\n",
       "      <td>359.997925</td>\n",
       "      <td>179.718684</td>\n",
       "      <td>179.515213</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>89.808235</td>\n",
       "      <td>269.683441</td>\n",
       "      <td>103.896367</td>\n",
       "      <td>4.969715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>0.833899</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>-0.003087</td>\n",
       "      <td>-0.859759</td>\n",
       "      <td>-0.178891</td>\n",
       "      <td>0.176823</td>\n",
       "      <td>0.254418</td>\n",
       "      <td>1.406934</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>-0.000923</td>\n",
       "      <td>...</td>\n",
       "      <td>14.678464</td>\n",
       "      <td>0.381383</td>\n",
       "      <td>359.984406</td>\n",
       "      <td>179.793080</td>\n",
       "      <td>179.564713</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>89.737358</td>\n",
       "      <td>269.943298</td>\n",
       "      <td>103.949458</td>\n",
       "      <td>4.822637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>0.758009</td>\n",
       "      <td>-0.000691</td>\n",
       "      <td>-0.001810</td>\n",
       "      <td>-0.905018</td>\n",
       "      <td>-0.180721</td>\n",
       "      <td>0.180212</td>\n",
       "      <td>0.250106</td>\n",
       "      <td>1.436152</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.006587</td>\n",
       "      <td>...</td>\n",
       "      <td>14.712968</td>\n",
       "      <td>0.416539</td>\n",
       "      <td>359.991730</td>\n",
       "      <td>179.912043</td>\n",
       "      <td>179.905869</td>\n",
       "      <td>0.033749</td>\n",
       "      <td>89.649193</td>\n",
       "      <td>269.937561</td>\n",
       "      <td>103.980746</td>\n",
       "      <td>4.997842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>1.214840</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.005317</td>\n",
       "      <td>-1.232988</td>\n",
       "      <td>-0.271643</td>\n",
       "      <td>0.270756</td>\n",
       "      <td>0.391789</td>\n",
       "      <td>1.565334</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.010769</td>\n",
       "      <td>...</td>\n",
       "      <td>14.715217</td>\n",
       "      <td>0.468821</td>\n",
       "      <td>359.988831</td>\n",
       "      <td>179.893793</td>\n",
       "      <td>179.870667</td>\n",
       "      <td>0.044999</td>\n",
       "      <td>90.053459</td>\n",
       "      <td>269.676636</td>\n",
       "      <td>103.809449</td>\n",
       "      <td>4.715598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15498 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      acceleration_CROSS_max  acceleration_CROSS_mean  \\\n",
       "1                   0.079123                -0.000025   \n",
       "8                   0.080641                 0.000008   \n",
       "13                  0.114809                -0.000008   \n",
       "14                  0.143640                 0.000045   \n",
       "20                  0.170798                 0.000066   \n",
       "...                      ...                      ...   \n",
       "1859                0.504826                 0.000088   \n",
       "1862                0.523959                 0.000091   \n",
       "1873                0.833899                -0.000500   \n",
       "1874                0.758009                -0.000691   \n",
       "1888                1.214840                -0.000119   \n",
       "\n",
       "      acceleration_CROSS_median  acceleration_CROSS_min  \\\n",
       "1                      0.000207               -0.081799   \n",
       "8                      0.000514               -0.079200   \n",
       "13                    -0.000328               -0.132331   \n",
       "14                     0.000258               -0.130101   \n",
       "20                    -0.000534               -0.176361   \n",
       "...                         ...                     ...   \n",
       "1859                  -0.000261               -0.540348   \n",
       "1862                  -0.000040               -0.599360   \n",
       "1873                  -0.003087               -0.859759   \n",
       "1874                  -0.001810               -0.905018   \n",
       "1888                  -0.005317               -1.232988   \n",
       "\n",
       "      acceleration_CROSS_q1  acceleration_CROSS_q3  acceleration_CROSS_std  \\\n",
       "1                 -0.006320               0.006222                0.016439   \n",
       "8                 -0.010629               0.010517                0.017438   \n",
       "13                -0.018883               0.018894                0.028989   \n",
       "14                -0.022333               0.022096                0.033276   \n",
       "20                -0.027387               0.027084                0.043117   \n",
       "...                     ...                    ...                     ...   \n",
       "1859              -0.127715               0.131784                0.176200   \n",
       "1862              -0.134737               0.138828                0.185669   \n",
       "1873              -0.178891               0.176823                0.254418   \n",
       "1874              -0.180721               0.180212                0.250106   \n",
       "1888              -0.271643               0.270756                0.391789   \n",
       "\n",
       "      acceleration_THRUST_max  acceleration_THRUST_mean  \\\n",
       "1                    0.098308                  0.000017   \n",
       "8                    0.162419                  0.000027   \n",
       "13                   0.204867                  0.000042   \n",
       "14                   0.293878                 -0.000093   \n",
       "20                   0.314227                 -0.000052   \n",
       "...                       ...                       ...   \n",
       "1859                 1.355515                  0.000265   \n",
       "1862                 1.356576                  0.000268   \n",
       "1873                 1.406934                  0.000386   \n",
       "1874                 1.436152                 -0.000137   \n",
       "1888                 1.565334                 -0.000075   \n",
       "\n",
       "      acceleration_THRUST_median  ...  rotor_speed_q3  rotor_speed_std  \\\n",
       "1                      -0.000257  ...        7.713003         0.059454   \n",
       "8                      -0.000233  ...       10.392641         0.360333   \n",
       "13                      0.000039  ...       13.281627         0.518050   \n",
       "14                     -0.000285  ...       13.899364         0.368430   \n",
       "20                     -0.000444  ...       14.411539         0.189976   \n",
       "...                          ...  ...             ...              ...   \n",
       "1859                    0.004446  ...       14.681202         0.343336   \n",
       "1862                    0.004309  ...       14.682295         0.347207   \n",
       "1873                   -0.000923  ...       14.678464         0.381383   \n",
       "1874                   -0.006587  ...       14.712968         0.416539   \n",
       "1888                   -0.010769  ...       14.715217         0.468821   \n",
       "\n",
       "      rotorposition_max  rotorposition_mean  rotorposition_median  \\\n",
       "1            359.996094          179.771935            179.570541   \n",
       "8            359.993561          180.052960            180.064270   \n",
       "13           359.995972          180.233785            180.515472   \n",
       "14           359.996155          180.218371            180.481812   \n",
       "20           359.998138          179.879823            179.830322   \n",
       "...                 ...                 ...                   ...   \n",
       "1859         359.985352          179.604842            179.351456   \n",
       "1862         359.997925          179.718684            179.515213   \n",
       "1873         359.984406          179.793080            179.564713   \n",
       "1874         359.991730          179.912043            179.905869   \n",
       "1888         359.988831          179.893793            179.870667   \n",
       "\n",
       "      rotorposition_min  rotorposition_q1  rotorposition_q3  \\\n",
       "1              0.033750         89.852768        269.434570   \n",
       "8              0.000000         90.110886        270.040161   \n",
       "13             0.000000         90.347740        270.210724   \n",
       "14             0.000000         90.269035        270.109589   \n",
       "20             0.011250         89.943283        269.852356   \n",
       "...                 ...               ...               ...   \n",
       "1859           0.011250         89.602600        269.572784   \n",
       "1862           0.045000         89.808235        269.683441   \n",
       "1873           0.011250         89.737358        269.943298   \n",
       "1874           0.033749         89.649193        269.937561   \n",
       "1888           0.044999         90.053459        269.676636   \n",
       "\n",
       "      rotorposition_std  yaw_error_mean  \n",
       "1            103.909795       -1.220241  \n",
       "8            103.919746       -1.081554  \n",
       "13           103.893912       -0.955399  \n",
       "14           103.856943       -0.905213  \n",
       "20           103.917794       -1.059410  \n",
       "...                 ...             ...  \n",
       "1859         103.891620        4.968577  \n",
       "1862         103.896367        4.969715  \n",
       "1873         103.949458        4.822637  \n",
       "1874         103.980746        4.997842  \n",
       "1888         103.809449        4.715598  \n",
       "\n",
       "[15498 rows x 71 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_test_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d508893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features for binning based on correlation analysis:\n",
      "['yaw_error_mean']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "correlation_matrix = big_train_updated.corr()\n",
    "\n",
    "correlation_with_target = correlation_matrix['yaw_error_mean'].abs()\n",
    "\n",
    "correlation_threshold = 0.5\n",
    "\n",
    "selected_features = correlation_with_target[correlation_with_target > correlation_threshold].index.tolist()\n",
    "\n",
    "print(\"Selected features for binning based on correlation analysis:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2797d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d7006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96093e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "combined_df = pd.concat([big_train_updated, big_test_updated], axis=0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "normalized_data = scaler.fit_transform(combined_df)\n",
    "\n",
    "normalized_train_df = pd.DataFrame(normalized_data[:len(big_train_updated)], columns=big_train_updated.columns)\n",
    "normalized_test_df = pd.DataFrame(normalized_data[len(big_train_updated):], columns=big_test_updated.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0241f302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceleration_CROSS_max</th>\n",
       "      <th>acceleration_CROSS_mean</th>\n",
       "      <th>acceleration_CROSS_median</th>\n",
       "      <th>acceleration_CROSS_min</th>\n",
       "      <th>acceleration_CROSS_q1</th>\n",
       "      <th>acceleration_CROSS_q3</th>\n",
       "      <th>acceleration_CROSS_std</th>\n",
       "      <th>acceleration_THRUST_max</th>\n",
       "      <th>acceleration_THRUST_mean</th>\n",
       "      <th>acceleration_THRUST_median</th>\n",
       "      <th>...</th>\n",
       "      <th>rotor_speed_q3</th>\n",
       "      <th>rotor_speed_std</th>\n",
       "      <th>rotorposition_max</th>\n",
       "      <th>rotorposition_mean</th>\n",
       "      <th>rotorposition_median</th>\n",
       "      <th>rotorposition_min</th>\n",
       "      <th>rotorposition_q1</th>\n",
       "      <th>rotorposition_q3</th>\n",
       "      <th>rotorposition_std</th>\n",
       "      <th>yaw_error_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.349366</td>\n",
       "      <td>0.557476</td>\n",
       "      <td>0.614840</td>\n",
       "      <td>0.737410</td>\n",
       "      <td>0.694435</td>\n",
       "      <td>0.308101</td>\n",
       "      <td>0.297861</td>\n",
       "      <td>0.548850</td>\n",
       "      <td>0.346788</td>\n",
       "      <td>0.479485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988970</td>\n",
       "      <td>0.221821</td>\n",
       "      <td>0.973409</td>\n",
       "      <td>0.314926</td>\n",
       "      <td>0.242694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350148</td>\n",
       "      <td>0.350599</td>\n",
       "      <td>0.331448</td>\n",
       "      <td>0.469812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153562</td>\n",
       "      <td>0.503807</td>\n",
       "      <td>0.515650</td>\n",
       "      <td>0.866259</td>\n",
       "      <td>0.859350</td>\n",
       "      <td>0.140526</td>\n",
       "      <td>0.135006</td>\n",
       "      <td>0.282869</td>\n",
       "      <td>0.491036</td>\n",
       "      <td>0.391224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982668</td>\n",
       "      <td>0.152858</td>\n",
       "      <td>0.985211</td>\n",
       "      <td>0.431097</td>\n",
       "      <td>0.411584</td>\n",
       "      <td>0.294117</td>\n",
       "      <td>0.585688</td>\n",
       "      <td>0.357765</td>\n",
       "      <td>0.281031</td>\n",
       "      <td>0.477019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015014</td>\n",
       "      <td>0.467738</td>\n",
       "      <td>0.540298</td>\n",
       "      <td>0.980354</td>\n",
       "      <td>0.954685</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>0.052206</td>\n",
       "      <td>0.459042</td>\n",
       "      <td>0.448536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317804</td>\n",
       "      <td>0.670279</td>\n",
       "      <td>0.997162</td>\n",
       "      <td>0.475088</td>\n",
       "      <td>0.454193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230174</td>\n",
       "      <td>0.722896</td>\n",
       "      <td>0.890997</td>\n",
       "      <td>0.467046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031675</td>\n",
       "      <td>0.463274</td>\n",
       "      <td>0.546874</td>\n",
       "      <td>0.979874</td>\n",
       "      <td>0.952247</td>\n",
       "      <td>0.046866</td>\n",
       "      <td>0.030507</td>\n",
       "      <td>0.065342</td>\n",
       "      <td>0.446193</td>\n",
       "      <td>0.438022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167050</td>\n",
       "      <td>0.304845</td>\n",
       "      <td>0.979086</td>\n",
       "      <td>0.612448</td>\n",
       "      <td>0.654564</td>\n",
       "      <td>0.058823</td>\n",
       "      <td>0.470929</td>\n",
       "      <td>0.727533</td>\n",
       "      <td>0.656860</td>\n",
       "      <td>0.489250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093985</td>\n",
       "      <td>0.503418</td>\n",
       "      <td>0.561773</td>\n",
       "      <td>0.919035</td>\n",
       "      <td>0.915027</td>\n",
       "      <td>0.084062</td>\n",
       "      <td>0.072291</td>\n",
       "      <td>0.162742</td>\n",
       "      <td>0.470128</td>\n",
       "      <td>0.473805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941950</td>\n",
       "      <td>0.435309</td>\n",
       "      <td>0.994473</td>\n",
       "      <td>0.618951</td>\n",
       "      <td>0.617797</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.471350</td>\n",
       "      <td>0.654775</td>\n",
       "      <td>0.662335</td>\n",
       "      <td>0.479649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62069</th>\n",
       "      <td>0.468178</td>\n",
       "      <td>0.582299</td>\n",
       "      <td>0.617354</td>\n",
       "      <td>0.606218</td>\n",
       "      <td>0.554040</td>\n",
       "      <td>0.427089</td>\n",
       "      <td>0.435614</td>\n",
       "      <td>0.476733</td>\n",
       "      <td>0.687955</td>\n",
       "      <td>0.412474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988090</td>\n",
       "      <td>0.202876</td>\n",
       "      <td>0.862115</td>\n",
       "      <td>0.330160</td>\n",
       "      <td>0.308144</td>\n",
       "      <td>0.117639</td>\n",
       "      <td>0.455668</td>\n",
       "      <td>0.310403</td>\n",
       "      <td>0.304896</td>\n",
       "      <td>0.618714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62070</th>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.468797</td>\n",
       "      <td>0.535523</td>\n",
       "      <td>0.989517</td>\n",
       "      <td>0.983301</td>\n",
       "      <td>0.016848</td>\n",
       "      <td>0.009951</td>\n",
       "      <td>0.043026</td>\n",
       "      <td>0.444364</td>\n",
       "      <td>0.460481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.057659</td>\n",
       "      <td>0.938602</td>\n",
       "      <td>0.571160</td>\n",
       "      <td>0.535903</td>\n",
       "      <td>0.058822</td>\n",
       "      <td>0.560288</td>\n",
       "      <td>0.582200</td>\n",
       "      <td>0.462811</td>\n",
       "      <td>0.614284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62071</th>\n",
       "      <td>0.514638</td>\n",
       "      <td>0.317059</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.561080</td>\n",
       "      <td>0.539785</td>\n",
       "      <td>0.443508</td>\n",
       "      <td>0.442799</td>\n",
       "      <td>0.679540</td>\n",
       "      <td>0.462570</td>\n",
       "      <td>0.663617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992909</td>\n",
       "      <td>0.244603</td>\n",
       "      <td>0.873319</td>\n",
       "      <td>0.432377</td>\n",
       "      <td>0.411520</td>\n",
       "      <td>0.058820</td>\n",
       "      <td>0.468037</td>\n",
       "      <td>0.471791</td>\n",
       "      <td>0.444710</td>\n",
       "      <td>0.615667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62072</th>\n",
       "      <td>0.053480</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>0.515802</td>\n",
       "      <td>0.960438</td>\n",
       "      <td>0.944735</td>\n",
       "      <td>0.054827</td>\n",
       "      <td>0.039321</td>\n",
       "      <td>0.077595</td>\n",
       "      <td>0.464556</td>\n",
       "      <td>0.483610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947921</td>\n",
       "      <td>0.081663</td>\n",
       "      <td>0.800568</td>\n",
       "      <td>0.473728</td>\n",
       "      <td>0.394397</td>\n",
       "      <td>0.352904</td>\n",
       "      <td>0.417801</td>\n",
       "      <td>0.700177</td>\n",
       "      <td>0.653788</td>\n",
       "      <td>0.616763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62073</th>\n",
       "      <td>0.171834</td>\n",
       "      <td>0.473388</td>\n",
       "      <td>0.505757</td>\n",
       "      <td>0.815094</td>\n",
       "      <td>0.804123</td>\n",
       "      <td>0.190970</td>\n",
       "      <td>0.174561</td>\n",
       "      <td>0.340865</td>\n",
       "      <td>0.682916</td>\n",
       "      <td>0.499681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986923</td>\n",
       "      <td>0.181465</td>\n",
       "      <td>0.990589</td>\n",
       "      <td>0.259115</td>\n",
       "      <td>0.252776</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.356652</td>\n",
       "      <td>0.308846</td>\n",
       "      <td>0.372816</td>\n",
       "      <td>0.617315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62074 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       acceleration_CROSS_max  acceleration_CROSS_mean  \\\n",
       "0                    0.349366                 0.557476   \n",
       "1                    0.153562                 0.503807   \n",
       "2                    0.015014                 0.467738   \n",
       "3                    0.031675                 0.463274   \n",
       "4                    0.093985                 0.503418   \n",
       "...                       ...                      ...   \n",
       "62069                0.468178                 0.582299   \n",
       "62070                0.012903                 0.468797   \n",
       "62071                0.514638                 0.317059   \n",
       "62072                0.053480                 0.472008   \n",
       "62073                0.171834                 0.473388   \n",
       "\n",
       "       acceleration_CROSS_median  acceleration_CROSS_min  \\\n",
       "0                       0.614840                0.737410   \n",
       "1                       0.515650                0.866259   \n",
       "2                       0.540298                0.980354   \n",
       "3                       0.546874                0.979874   \n",
       "4                       0.561773                0.919035   \n",
       "...                          ...                     ...   \n",
       "62069                   0.617354                0.606218   \n",
       "62070                   0.535523                0.989517   \n",
       "62071                   0.607143                0.561080   \n",
       "62072                   0.515802                0.960438   \n",
       "62073                   0.505757                0.815094   \n",
       "\n",
       "       acceleration_CROSS_q1  acceleration_CROSS_q3  acceleration_CROSS_std  \\\n",
       "0                   0.694435               0.308101                0.297861   \n",
       "1                   0.859350               0.140526                0.135006   \n",
       "2                   0.954685               0.045100                0.025532   \n",
       "3                   0.952247               0.046866                0.030507   \n",
       "4                   0.915027               0.084062                0.072291   \n",
       "...                      ...                    ...                     ...   \n",
       "62069               0.554040               0.427089                0.435614   \n",
       "62070               0.983301               0.016848                0.009951   \n",
       "62071               0.539785               0.443508                0.442799   \n",
       "62072               0.944735               0.054827                0.039321   \n",
       "62073               0.804123               0.190970                0.174561   \n",
       "\n",
       "       acceleration_THRUST_max  acceleration_THRUST_mean  \\\n",
       "0                     0.548850                  0.346788   \n",
       "1                     0.282869                  0.491036   \n",
       "2                     0.052206                  0.459042   \n",
       "3                     0.065342                  0.446193   \n",
       "4                     0.162742                  0.470128   \n",
       "...                        ...                       ...   \n",
       "62069                 0.476733                  0.687955   \n",
       "62070                 0.043026                  0.444364   \n",
       "62071                 0.679540                  0.462570   \n",
       "62072                 0.077595                  0.464556   \n",
       "62073                 0.340865                  0.682916   \n",
       "\n",
       "       acceleration_THRUST_median  ...  rotor_speed_q3  rotor_speed_std  \\\n",
       "0                        0.479485  ...        0.988970         0.221821   \n",
       "1                        0.391224  ...        0.982668         0.152858   \n",
       "2                        0.448536  ...        0.317804         0.670279   \n",
       "3                        0.438022  ...        0.167050         0.304845   \n",
       "4                        0.473805  ...        0.941950         0.435309   \n",
       "...                           ...  ...             ...              ...   \n",
       "62069                    0.412474  ...        0.988090         0.202876   \n",
       "62070                    0.460481  ...        0.015038         0.057659   \n",
       "62071                    0.663617  ...        0.992909         0.244603   \n",
       "62072                    0.483610  ...        0.947921         0.081663   \n",
       "62073                    0.499681  ...        0.986923         0.181465   \n",
       "\n",
       "       rotorposition_max  rotorposition_mean  rotorposition_median  \\\n",
       "0               0.973409            0.314926              0.242694   \n",
       "1               0.985211            0.431097              0.411584   \n",
       "2               0.997162            0.475088              0.454193   \n",
       "3               0.979086            0.612448              0.654564   \n",
       "4               0.994473            0.618951              0.617797   \n",
       "...                  ...                 ...                   ...   \n",
       "62069           0.862115            0.330160              0.308144   \n",
       "62070           0.938602            0.571160              0.535903   \n",
       "62071           0.873319            0.432377              0.411520   \n",
       "62072           0.800568            0.473728              0.394397   \n",
       "62073           0.990589            0.259115              0.252776   \n",
       "\n",
       "       rotorposition_min  rotorposition_q1  rotorposition_q3  \\\n",
       "0               0.000000          0.350148          0.350599   \n",
       "1               0.294117          0.585688          0.357765   \n",
       "2               0.000000          0.230174          0.722896   \n",
       "3               0.058823          0.470929          0.727533   \n",
       "4               0.058824          0.471350          0.654775   \n",
       "...                  ...               ...               ...   \n",
       "62069           0.117639          0.455668          0.310403   \n",
       "62070           0.058822          0.560288          0.582200   \n",
       "62071           0.058820          0.468037          0.471791   \n",
       "62072           0.352904          0.417801          0.700177   \n",
       "62073           0.117647          0.356652          0.308846   \n",
       "\n",
       "       rotorposition_std  yaw_error_mean  \n",
       "0               0.331448        0.469812  \n",
       "1               0.281031        0.477019  \n",
       "2               0.890997        0.467046  \n",
       "3               0.656860        0.489250  \n",
       "4               0.662335        0.479649  \n",
       "...                  ...             ...  \n",
       "62069           0.304896        0.618714  \n",
       "62070           0.462811        0.614284  \n",
       "62071           0.444710        0.615667  \n",
       "62072           0.653788        0.616763  \n",
       "62073           0.372816        0.617315  \n",
       "\n",
       "[62074 rows x 71 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60008baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2671559401456/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2671559401456/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2671970964800/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2671970964800/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/2671617380784/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/2671617380784/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from interpret import show\n",
    "from interpret.data import Marginal\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "\n",
    "\n",
    "X_train = normalized_train_df.drop('yaw_error_mean', axis=1)\n",
    "y_train = normalized_train_df['yaw_error_mean']\n",
    "\n",
    "ebr = ExplainableBoostingRegressor()\n",
    "ebr.fit(X_train, y_train)\n",
    "\n",
    "feature_importance = Marginal().explain_data(X_train, y_train, name='Feature Importance')\n",
    "\n",
    "show(feature_importance)\n",
    "\n",
    "show(ebr.explain_global())\n",
    "show(ebr.explain_local(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560567fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42901bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b77d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b500993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n",
      "C:\\Users\\musab\\AppData\\Local\\Temp\\ipykernel_11324\\3636247876.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[exponential_columns] = exponential_features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.078607\n",
      "0:\tlearn: 0.2800895\ttotal: 241ms\tremaining: 4m 1s\n",
      "1:\tlearn: 0.2746249\ttotal: 283ms\tremaining: 2m 21s\n",
      "2:\tlearn: 0.2699766\ttotal: 323ms\tremaining: 1m 47s\n",
      "3:\tlearn: 0.2655539\ttotal: 366ms\tremaining: 1m 31s\n",
      "4:\tlearn: 0.2617911\ttotal: 408ms\tremaining: 1m 21s\n",
      "5:\tlearn: 0.2585551\ttotal: 451ms\tremaining: 1m 14s\n",
      "6:\tlearn: 0.2554607\ttotal: 490ms\tremaining: 1m 9s\n",
      "7:\tlearn: 0.2523390\ttotal: 537ms\tremaining: 1m 6s\n",
      "8:\tlearn: 0.2494432\ttotal: 582ms\tremaining: 1m 4s\n",
      "9:\tlearn: 0.2471385\ttotal: 624ms\tremaining: 1m 1s\n",
      "10:\tlearn: 0.2450296\ttotal: 662ms\tremaining: 59.5s\n",
      "11:\tlearn: 0.2429219\ttotal: 705ms\tremaining: 58s\n",
      "12:\tlearn: 0.2412139\ttotal: 749ms\tremaining: 56.8s\n",
      "13:\tlearn: 0.2392575\ttotal: 800ms\tremaining: 56.3s\n",
      "14:\tlearn: 0.2373173\ttotal: 844ms\tremaining: 55.4s\n",
      "15:\tlearn: 0.2358031\ttotal: 889ms\tremaining: 54.7s\n",
      "16:\tlearn: 0.2341817\ttotal: 934ms\tremaining: 54s\n",
      "17:\tlearn: 0.2328958\ttotal: 975ms\tremaining: 53.2s\n",
      "18:\tlearn: 0.2316220\ttotal: 1.02s\tremaining: 52.7s\n",
      "19:\tlearn: 0.2300495\ttotal: 1.07s\tremaining: 52.4s\n",
      "20:\tlearn: 0.2290168\ttotal: 1.11s\tremaining: 51.6s\n",
      "21:\tlearn: 0.2281756\ttotal: 1.15s\tremaining: 50.9s\n",
      "22:\tlearn: 0.2268388\ttotal: 1.18s\tremaining: 50.2s\n",
      "23:\tlearn: 0.2258055\ttotal: 1.23s\tremaining: 49.9s\n",
      "24:\tlearn: 0.2245505\ttotal: 1.26s\tremaining: 49.3s\n",
      "25:\tlearn: 0.2235156\ttotal: 1.31s\tremaining: 49.2s\n",
      "26:\tlearn: 0.2227872\ttotal: 1.35s\tremaining: 48.6s\n",
      "27:\tlearn: 0.2217813\ttotal: 1.39s\tremaining: 48.2s\n",
      "28:\tlearn: 0.2207859\ttotal: 1.43s\tremaining: 48s\n",
      "29:\tlearn: 0.2200154\ttotal: 1.48s\tremaining: 47.7s\n",
      "30:\tlearn: 0.2193855\ttotal: 1.52s\tremaining: 47.4s\n",
      "31:\tlearn: 0.2184727\ttotal: 1.56s\tremaining: 47.1s\n",
      "32:\tlearn: 0.2175009\ttotal: 1.59s\tremaining: 46.8s\n",
      "33:\tlearn: 0.2169407\ttotal: 1.63s\tremaining: 46.4s\n",
      "34:\tlearn: 0.2161005\ttotal: 1.67s\tremaining: 46s\n",
      "35:\tlearn: 0.2153814\ttotal: 1.71s\tremaining: 45.8s\n",
      "36:\tlearn: 0.2146041\ttotal: 1.75s\tremaining: 45.7s\n",
      "37:\tlearn: 0.2135063\ttotal: 1.79s\tremaining: 45.3s\n",
      "38:\tlearn: 0.2127081\ttotal: 1.83s\tremaining: 45.2s\n",
      "39:\tlearn: 0.2119873\ttotal: 1.88s\tremaining: 45.1s\n",
      "40:\tlearn: 0.2113988\ttotal: 1.92s\tremaining: 44.8s\n",
      "41:\tlearn: 0.2108972\ttotal: 1.95s\tremaining: 44.5s\n",
      "42:\tlearn: 0.2100767\ttotal: 2s\tremaining: 44.5s\n",
      "43:\tlearn: 0.2094673\ttotal: 2.04s\tremaining: 44.3s\n",
      "44:\tlearn: 0.2087979\ttotal: 2.08s\tremaining: 44.2s\n",
      "45:\tlearn: 0.2080723\ttotal: 2.12s\tremaining: 43.9s\n",
      "46:\tlearn: 0.2076269\ttotal: 2.15s\tremaining: 43.7s\n",
      "47:\tlearn: 0.2070821\ttotal: 2.19s\tremaining: 43.5s\n",
      "48:\tlearn: 0.2065049\ttotal: 2.24s\tremaining: 43.4s\n",
      "49:\tlearn: 0.2060007\ttotal: 2.27s\tremaining: 43.2s\n",
      "50:\tlearn: 0.2055351\ttotal: 2.32s\tremaining: 43.2s\n",
      "51:\tlearn: 0.2050474\ttotal: 2.36s\tremaining: 43.1s\n",
      "52:\tlearn: 0.2046318\ttotal: 2.4s\tremaining: 42.9s\n",
      "53:\tlearn: 0.2039523\ttotal: 2.44s\tremaining: 42.7s\n",
      "54:\tlearn: 0.2031419\ttotal: 2.5s\tremaining: 43s\n",
      "55:\tlearn: 0.2027502\ttotal: 2.54s\tremaining: 42.8s\n",
      "56:\tlearn: 0.2021646\ttotal: 2.59s\tremaining: 42.8s\n",
      "57:\tlearn: 0.2016984\ttotal: 2.63s\tremaining: 42.6s\n",
      "58:\tlearn: 0.2009294\ttotal: 2.67s\tremaining: 42.5s\n",
      "59:\tlearn: 0.2002326\ttotal: 2.71s\tremaining: 42.5s\n",
      "60:\tlearn: 0.1996860\ttotal: 2.76s\tremaining: 42.5s\n",
      "61:\tlearn: 0.1989943\ttotal: 2.8s\tremaining: 42.4s\n",
      "62:\tlearn: 0.1985395\ttotal: 2.84s\tremaining: 42.3s\n",
      "63:\tlearn: 0.1980199\ttotal: 2.88s\tremaining: 42.2s\n",
      "64:\tlearn: 0.1976162\ttotal: 2.93s\tremaining: 42.1s\n",
      "65:\tlearn: 0.1971141\ttotal: 2.98s\tremaining: 42.1s\n",
      "66:\tlearn: 0.1966747\ttotal: 3.02s\tremaining: 42.1s\n",
      "67:\tlearn: 0.1963214\ttotal: 3.06s\tremaining: 41.9s\n",
      "68:\tlearn: 0.1955026\ttotal: 3.11s\tremaining: 42s\n",
      "69:\tlearn: 0.1950986\ttotal: 3.15s\tremaining: 41.8s\n",
      "70:\tlearn: 0.1947600\ttotal: 3.19s\tremaining: 41.7s\n",
      "71:\tlearn: 0.1942971\ttotal: 3.23s\tremaining: 41.7s\n",
      "72:\tlearn: 0.1938554\ttotal: 3.27s\tremaining: 41.5s\n",
      "73:\tlearn: 0.1931849\ttotal: 3.31s\tremaining: 41.5s\n",
      "74:\tlearn: 0.1929072\ttotal: 3.35s\tremaining: 41.3s\n",
      "75:\tlearn: 0.1924358\ttotal: 3.4s\tremaining: 41.3s\n",
      "76:\tlearn: 0.1921493\ttotal: 3.44s\tremaining: 41.2s\n",
      "77:\tlearn: 0.1915253\ttotal: 3.49s\tremaining: 41.2s\n",
      "78:\tlearn: 0.1911798\ttotal: 3.53s\tremaining: 41.2s\n",
      "79:\tlearn: 0.1908744\ttotal: 3.58s\tremaining: 41.2s\n",
      "80:\tlearn: 0.1905981\ttotal: 3.63s\tremaining: 41.2s\n",
      "81:\tlearn: 0.1904184\ttotal: 3.67s\tremaining: 41.1s\n",
      "82:\tlearn: 0.1898513\ttotal: 3.71s\tremaining: 41s\n",
      "83:\tlearn: 0.1893510\ttotal: 3.75s\tremaining: 40.9s\n",
      "84:\tlearn: 0.1890350\ttotal: 3.79s\tremaining: 40.9s\n",
      "85:\tlearn: 0.1887033\ttotal: 3.84s\tremaining: 40.8s\n",
      "86:\tlearn: 0.1880515\ttotal: 3.88s\tremaining: 40.7s\n",
      "87:\tlearn: 0.1875330\ttotal: 3.93s\tremaining: 40.7s\n",
      "88:\tlearn: 0.1871822\ttotal: 3.97s\tremaining: 40.6s\n",
      "89:\tlearn: 0.1867671\ttotal: 4.01s\tremaining: 40.5s\n",
      "90:\tlearn: 0.1865585\ttotal: 4.05s\tremaining: 40.5s\n",
      "91:\tlearn: 0.1860821\ttotal: 4.1s\tremaining: 40.5s\n",
      "92:\tlearn: 0.1857438\ttotal: 4.14s\tremaining: 40.4s\n",
      "93:\tlearn: 0.1851016\ttotal: 4.18s\tremaining: 40.3s\n",
      "94:\tlearn: 0.1846947\ttotal: 4.23s\tremaining: 40.3s\n",
      "95:\tlearn: 0.1843375\ttotal: 4.28s\tremaining: 40.3s\n",
      "96:\tlearn: 0.1841287\ttotal: 4.31s\tremaining: 40.1s\n",
      "97:\tlearn: 0.1837090\ttotal: 4.36s\tremaining: 40.2s\n",
      "98:\tlearn: 0.1832310\ttotal: 4.41s\tremaining: 40.1s\n",
      "99:\tlearn: 0.1829205\ttotal: 4.45s\tremaining: 40.1s\n",
      "100:\tlearn: 0.1824222\ttotal: 4.5s\tremaining: 40.1s\n",
      "101:\tlearn: 0.1821857\ttotal: 4.55s\tremaining: 40s\n",
      "102:\tlearn: 0.1817133\ttotal: 4.59s\tremaining: 40s\n",
      "103:\tlearn: 0.1813629\ttotal: 4.63s\tremaining: 39.9s\n",
      "104:\tlearn: 0.1809532\ttotal: 4.68s\tremaining: 39.9s\n",
      "105:\tlearn: 0.1806692\ttotal: 4.73s\tremaining: 39.9s\n",
      "106:\tlearn: 0.1804170\ttotal: 4.77s\tremaining: 39.8s\n",
      "107:\tlearn: 0.1799953\ttotal: 4.82s\tremaining: 39.8s\n",
      "108:\tlearn: 0.1795830\ttotal: 4.86s\tremaining: 39.7s\n",
      "109:\tlearn: 0.1791200\ttotal: 4.9s\tremaining: 39.6s\n",
      "110:\tlearn: 0.1787803\ttotal: 4.93s\tremaining: 39.5s\n",
      "111:\tlearn: 0.1783991\ttotal: 4.98s\tremaining: 39.5s\n",
      "112:\tlearn: 0.1782120\ttotal: 5.02s\tremaining: 39.4s\n",
      "113:\tlearn: 0.1779602\ttotal: 5.06s\tremaining: 39.3s\n",
      "114:\tlearn: 0.1776839\ttotal: 5.11s\tremaining: 39.3s\n",
      "115:\tlearn: 0.1772959\ttotal: 5.15s\tremaining: 39.2s\n",
      "116:\tlearn: 0.1769521\ttotal: 5.18s\tremaining: 39.1s\n",
      "117:\tlearn: 0.1767777\ttotal: 5.23s\tremaining: 39.1s\n",
      "118:\tlearn: 0.1763683\ttotal: 5.27s\tremaining: 39s\n",
      "119:\tlearn: 0.1761079\ttotal: 5.32s\tremaining: 39s\n",
      "120:\tlearn: 0.1757380\ttotal: 5.36s\tremaining: 39s\n",
      "121:\tlearn: 0.1753508\ttotal: 5.4s\tremaining: 38.9s\n",
      "122:\tlearn: 0.1750367\ttotal: 5.44s\tremaining: 38.8s\n",
      "123:\tlearn: 0.1747470\ttotal: 5.48s\tremaining: 38.7s\n",
      "124:\tlearn: 0.1743346\ttotal: 5.53s\tremaining: 38.7s\n",
      "125:\tlearn: 0.1739815\ttotal: 5.57s\tremaining: 38.7s\n",
      "126:\tlearn: 0.1737734\ttotal: 5.61s\tremaining: 38.6s\n",
      "127:\tlearn: 0.1734957\ttotal: 5.66s\tremaining: 38.5s\n",
      "128:\tlearn: 0.1733002\ttotal: 5.7s\tremaining: 38.5s\n",
      "129:\tlearn: 0.1730606\ttotal: 5.74s\tremaining: 38.4s\n",
      "130:\tlearn: 0.1726945\ttotal: 5.78s\tremaining: 38.3s\n",
      "131:\tlearn: 0.1723318\ttotal: 5.83s\tremaining: 38.4s\n",
      "132:\tlearn: 0.1720773\ttotal: 5.88s\tremaining: 38.3s\n",
      "133:\tlearn: 0.1717700\ttotal: 5.93s\tremaining: 38.3s\n",
      "134:\tlearn: 0.1715885\ttotal: 5.97s\tremaining: 38.2s\n",
      "135:\tlearn: 0.1712436\ttotal: 6.02s\tremaining: 38.2s\n",
      "136:\tlearn: 0.1710930\ttotal: 6.06s\tremaining: 38.2s\n",
      "137:\tlearn: 0.1708418\ttotal: 6.1s\tremaining: 38.1s\n",
      "138:\tlearn: 0.1705229\ttotal: 6.14s\tremaining: 38s\n",
      "139:\tlearn: 0.1701874\ttotal: 6.18s\tremaining: 37.9s\n",
      "140:\tlearn: 0.1699125\ttotal: 6.22s\tremaining: 37.9s\n",
      "141:\tlearn: 0.1695887\ttotal: 6.26s\tremaining: 37.8s\n",
      "142:\tlearn: 0.1693467\ttotal: 6.3s\tremaining: 37.8s\n",
      "143:\tlearn: 0.1691312\ttotal: 6.35s\tremaining: 37.7s\n",
      "144:\tlearn: 0.1688969\ttotal: 6.39s\tremaining: 37.7s\n",
      "145:\tlearn: 0.1687872\ttotal: 6.42s\tremaining: 37.6s\n",
      "146:\tlearn: 0.1685011\ttotal: 6.46s\tremaining: 37.5s\n",
      "147:\tlearn: 0.1682888\ttotal: 6.5s\tremaining: 37.4s\n",
      "148:\tlearn: 0.1679837\ttotal: 6.55s\tremaining: 37.4s\n",
      "149:\tlearn: 0.1677722\ttotal: 6.58s\tremaining: 37.3s\n",
      "150:\tlearn: 0.1675475\ttotal: 6.63s\tremaining: 37.3s\n",
      "151:\tlearn: 0.1674308\ttotal: 6.67s\tremaining: 37.2s\n",
      "152:\tlearn: 0.1670504\ttotal: 6.72s\tremaining: 37.2s\n",
      "153:\tlearn: 0.1668019\ttotal: 6.76s\tremaining: 37.1s\n",
      "154:\tlearn: 0.1665996\ttotal: 6.81s\tremaining: 37.1s\n",
      "155:\tlearn: 0.1663130\ttotal: 6.87s\tremaining: 37.2s\n",
      "156:\tlearn: 0.1660615\ttotal: 6.93s\tremaining: 37.2s\n",
      "157:\tlearn: 0.1658711\ttotal: 6.98s\tremaining: 37.2s\n",
      "158:\tlearn: 0.1656001\ttotal: 7.04s\tremaining: 37.2s\n",
      "159:\tlearn: 0.1653825\ttotal: 7.08s\tremaining: 37.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160:\tlearn: 0.1651716\ttotal: 7.13s\tremaining: 37.1s\n",
      "161:\tlearn: 0.1649661\ttotal: 7.17s\tremaining: 37.1s\n",
      "162:\tlearn: 0.1647079\ttotal: 7.21s\tremaining: 37s\n",
      "163:\tlearn: 0.1645246\ttotal: 7.25s\tremaining: 37s\n",
      "164:\tlearn: 0.1642029\ttotal: 7.29s\tremaining: 36.9s\n",
      "165:\tlearn: 0.1639066\ttotal: 7.35s\tremaining: 36.9s\n",
      "166:\tlearn: 0.1636469\ttotal: 7.39s\tremaining: 36.9s\n",
      "167:\tlearn: 0.1633391\ttotal: 7.44s\tremaining: 36.8s\n",
      "168:\tlearn: 0.1631439\ttotal: 7.48s\tremaining: 36.8s\n",
      "169:\tlearn: 0.1629601\ttotal: 7.52s\tremaining: 36.7s\n",
      "170:\tlearn: 0.1627350\ttotal: 7.57s\tremaining: 36.7s\n",
      "171:\tlearn: 0.1624936\ttotal: 7.61s\tremaining: 36.6s\n",
      "172:\tlearn: 0.1622663\ttotal: 7.64s\tremaining: 36.5s\n",
      "173:\tlearn: 0.1619821\ttotal: 7.69s\tremaining: 36.5s\n",
      "174:\tlearn: 0.1617887\ttotal: 7.74s\tremaining: 36.5s\n",
      "175:\tlearn: 0.1615927\ttotal: 7.79s\tremaining: 36.5s\n",
      "176:\tlearn: 0.1613970\ttotal: 7.83s\tremaining: 36.4s\n",
      "177:\tlearn: 0.1612214\ttotal: 7.88s\tremaining: 36.4s\n",
      "178:\tlearn: 0.1609451\ttotal: 7.92s\tremaining: 36.3s\n",
      "179:\tlearn: 0.1607200\ttotal: 7.97s\tremaining: 36.3s\n",
      "180:\tlearn: 0.1605694\ttotal: 8.01s\tremaining: 36.2s\n",
      "181:\tlearn: 0.1603973\ttotal: 8.05s\tremaining: 36.2s\n",
      "182:\tlearn: 0.1602837\ttotal: 8.09s\tremaining: 36.1s\n",
      "183:\tlearn: 0.1601934\ttotal: 8.13s\tremaining: 36s\n",
      "184:\tlearn: 0.1600187\ttotal: 8.17s\tremaining: 36s\n",
      "185:\tlearn: 0.1598355\ttotal: 8.21s\tremaining: 35.9s\n",
      "186:\tlearn: 0.1596695\ttotal: 8.26s\tremaining: 35.9s\n",
      "187:\tlearn: 0.1594241\ttotal: 8.31s\tremaining: 35.9s\n",
      "188:\tlearn: 0.1592384\ttotal: 8.35s\tremaining: 35.8s\n",
      "189:\tlearn: 0.1589717\ttotal: 8.39s\tremaining: 35.8s\n",
      "190:\tlearn: 0.1588055\ttotal: 8.44s\tremaining: 35.8s\n",
      "191:\tlearn: 0.1585807\ttotal: 8.49s\tremaining: 35.7s\n",
      "192:\tlearn: 0.1584100\ttotal: 8.53s\tremaining: 35.7s\n",
      "193:\tlearn: 0.1582915\ttotal: 8.57s\tremaining: 35.6s\n",
      "194:\tlearn: 0.1580706\ttotal: 8.62s\tremaining: 35.6s\n",
      "195:\tlearn: 0.1579063\ttotal: 8.66s\tremaining: 35.5s\n",
      "196:\tlearn: 0.1577259\ttotal: 8.7s\tremaining: 35.5s\n",
      "197:\tlearn: 0.1575344\ttotal: 8.75s\tremaining: 35.4s\n",
      "198:\tlearn: 0.1573652\ttotal: 8.8s\tremaining: 35.4s\n",
      "199:\tlearn: 0.1571983\ttotal: 8.84s\tremaining: 35.3s\n",
      "200:\tlearn: 0.1569550\ttotal: 8.88s\tremaining: 35.3s\n",
      "201:\tlearn: 0.1568055\ttotal: 8.91s\tremaining: 35.2s\n",
      "202:\tlearn: 0.1566185\ttotal: 8.96s\tremaining: 35.2s\n",
      "203:\tlearn: 0.1564322\ttotal: 9.01s\tremaining: 35.1s\n",
      "204:\tlearn: 0.1562435\ttotal: 9.05s\tremaining: 35.1s\n",
      "205:\tlearn: 0.1560750\ttotal: 9.09s\tremaining: 35s\n",
      "206:\tlearn: 0.1557871\ttotal: 9.13s\tremaining: 35s\n",
      "207:\tlearn: 0.1556289\ttotal: 9.18s\tremaining: 34.9s\n",
      "208:\tlearn: 0.1555441\ttotal: 9.21s\tremaining: 34.9s\n",
      "209:\tlearn: 0.1553028\ttotal: 9.26s\tremaining: 34.9s\n",
      "210:\tlearn: 0.1551495\ttotal: 9.3s\tremaining: 34.8s\n",
      "211:\tlearn: 0.1549889\ttotal: 9.35s\tremaining: 34.8s\n",
      "212:\tlearn: 0.1548248\ttotal: 9.39s\tremaining: 34.7s\n",
      "213:\tlearn: 0.1546366\ttotal: 9.43s\tremaining: 34.6s\n",
      "214:\tlearn: 0.1543282\ttotal: 9.48s\tremaining: 34.6s\n",
      "215:\tlearn: 0.1541428\ttotal: 9.52s\tremaining: 34.6s\n",
      "216:\tlearn: 0.1539295\ttotal: 9.57s\tremaining: 34.5s\n",
      "217:\tlearn: 0.1537306\ttotal: 9.61s\tremaining: 34.5s\n",
      "218:\tlearn: 0.1535986\ttotal: 9.65s\tremaining: 34.4s\n",
      "219:\tlearn: 0.1534919\ttotal: 9.69s\tremaining: 34.3s\n",
      "220:\tlearn: 0.1533051\ttotal: 9.73s\tremaining: 34.3s\n",
      "221:\tlearn: 0.1531366\ttotal: 9.77s\tremaining: 34.2s\n",
      "222:\tlearn: 0.1529306\ttotal: 9.82s\tremaining: 34.2s\n",
      "223:\tlearn: 0.1527726\ttotal: 9.86s\tremaining: 34.2s\n",
      "224:\tlearn: 0.1525775\ttotal: 9.9s\tremaining: 34.1s\n",
      "225:\tlearn: 0.1524124\ttotal: 9.94s\tremaining: 34s\n",
      "226:\tlearn: 0.1523123\ttotal: 9.98s\tremaining: 34s\n",
      "227:\tlearn: 0.1521761\ttotal: 10s\tremaining: 34s\n",
      "228:\tlearn: 0.1519966\ttotal: 10.1s\tremaining: 33.9s\n",
      "229:\tlearn: 0.1518492\ttotal: 10.1s\tremaining: 33.8s\n",
      "230:\tlearn: 0.1516013\ttotal: 10.1s\tremaining: 33.8s\n",
      "231:\tlearn: 0.1514532\ttotal: 10.2s\tremaining: 33.7s\n",
      "232:\tlearn: 0.1513244\ttotal: 10.2s\tremaining: 33.7s\n",
      "233:\tlearn: 0.1511913\ttotal: 10.3s\tremaining: 33.6s\n",
      "234:\tlearn: 0.1510311\ttotal: 10.3s\tremaining: 33.6s\n",
      "235:\tlearn: 0.1508604\ttotal: 10.4s\tremaining: 33.5s\n",
      "236:\tlearn: 0.1507170\ttotal: 10.4s\tremaining: 33.5s\n",
      "237:\tlearn: 0.1505138\ttotal: 10.4s\tremaining: 33.4s\n",
      "238:\tlearn: 0.1504150\ttotal: 10.5s\tremaining: 33.4s\n",
      "239:\tlearn: 0.1502446\ttotal: 10.5s\tremaining: 33.4s\n",
      "240:\tlearn: 0.1500938\ttotal: 10.6s\tremaining: 33.3s\n",
      "241:\tlearn: 0.1499396\ttotal: 10.6s\tremaining: 33.3s\n",
      "242:\tlearn: 0.1497053\ttotal: 10.7s\tremaining: 33.2s\n",
      "243:\tlearn: 0.1496153\ttotal: 10.7s\tremaining: 33.1s\n",
      "244:\tlearn: 0.1495409\ttotal: 10.7s\tremaining: 33.1s\n",
      "245:\tlearn: 0.1494276\ttotal: 10.8s\tremaining: 33s\n",
      "246:\tlearn: 0.1492270\ttotal: 10.8s\tremaining: 33s\n",
      "247:\tlearn: 0.1491084\ttotal: 10.9s\tremaining: 33s\n",
      "248:\tlearn: 0.1490061\ttotal: 10.9s\tremaining: 32.9s\n",
      "249:\tlearn: 0.1488338\ttotal: 11s\tremaining: 32.9s\n",
      "250:\tlearn: 0.1486916\ttotal: 11s\tremaining: 32.8s\n",
      "251:\tlearn: 0.1485506\ttotal: 11s\tremaining: 32.8s\n",
      "252:\tlearn: 0.1483882\ttotal: 11.1s\tremaining: 32.7s\n",
      "253:\tlearn: 0.1481918\ttotal: 11.1s\tremaining: 32.7s\n",
      "254:\tlearn: 0.1480592\ttotal: 11.2s\tremaining: 32.7s\n",
      "255:\tlearn: 0.1477942\ttotal: 11.2s\tremaining: 32.6s\n",
      "256:\tlearn: 0.1475881\ttotal: 11.3s\tremaining: 32.6s\n",
      "257:\tlearn: 0.1474242\ttotal: 11.3s\tremaining: 32.6s\n",
      "258:\tlearn: 0.1472963\ttotal: 11.4s\tremaining: 32.5s\n",
      "259:\tlearn: 0.1471226\ttotal: 11.4s\tremaining: 32.5s\n",
      "260:\tlearn: 0.1469719\ttotal: 11.5s\tremaining: 32.5s\n",
      "261:\tlearn: 0.1469048\ttotal: 11.5s\tremaining: 32.4s\n",
      "262:\tlearn: 0.1467917\ttotal: 11.5s\tremaining: 32.3s\n",
      "263:\tlearn: 0.1467397\ttotal: 11.6s\tremaining: 32.3s\n",
      "264:\tlearn: 0.1465736\ttotal: 11.6s\tremaining: 32.2s\n",
      "265:\tlearn: 0.1464263\ttotal: 11.7s\tremaining: 32.2s\n",
      "266:\tlearn: 0.1463042\ttotal: 11.7s\tremaining: 32.1s\n",
      "267:\tlearn: 0.1461997\ttotal: 11.7s\tremaining: 32.1s\n",
      "268:\tlearn: 0.1460415\ttotal: 11.8s\tremaining: 32s\n",
      "269:\tlearn: 0.1459524\ttotal: 11.8s\tremaining: 32s\n",
      "270:\tlearn: 0.1458255\ttotal: 11.9s\tremaining: 31.9s\n",
      "271:\tlearn: 0.1457509\ttotal: 11.9s\tremaining: 31.9s\n",
      "272:\tlearn: 0.1455850\ttotal: 12s\tremaining: 31.8s\n",
      "273:\tlearn: 0.1454481\ttotal: 12s\tremaining: 31.8s\n",
      "274:\tlearn: 0.1452652\ttotal: 12s\tremaining: 31.8s\n",
      "275:\tlearn: 0.1451362\ttotal: 12.1s\tremaining: 31.7s\n",
      "276:\tlearn: 0.1450216\ttotal: 12.1s\tremaining: 31.7s\n",
      "277:\tlearn: 0.1449066\ttotal: 12.2s\tremaining: 31.6s\n",
      "278:\tlearn: 0.1447390\ttotal: 12.2s\tremaining: 31.6s\n",
      "279:\tlearn: 0.1446297\ttotal: 12.3s\tremaining: 31.5s\n",
      "280:\tlearn: 0.1444930\ttotal: 12.3s\tremaining: 31.5s\n",
      "281:\tlearn: 0.1443766\ttotal: 12.3s\tremaining: 31.4s\n",
      "282:\tlearn: 0.1442804\ttotal: 12.4s\tremaining: 31.4s\n",
      "283:\tlearn: 0.1441672\ttotal: 12.4s\tremaining: 31.3s\n",
      "284:\tlearn: 0.1440509\ttotal: 12.5s\tremaining: 31.3s\n",
      "285:\tlearn: 0.1438982\ttotal: 12.5s\tremaining: 31.3s\n",
      "286:\tlearn: 0.1437958\ttotal: 12.6s\tremaining: 31.2s\n",
      "287:\tlearn: 0.1437223\ttotal: 12.6s\tremaining: 31.1s\n",
      "288:\tlearn: 0.1435734\ttotal: 12.6s\tremaining: 31.1s\n",
      "289:\tlearn: 0.1434244\ttotal: 12.7s\tremaining: 31.1s\n",
      "290:\tlearn: 0.1433363\ttotal: 12.7s\tremaining: 31s\n",
      "291:\tlearn: 0.1432468\ttotal: 12.8s\tremaining: 31s\n",
      "292:\tlearn: 0.1430888\ttotal: 12.8s\tremaining: 30.9s\n",
      "293:\tlearn: 0.1429603\ttotal: 12.9s\tremaining: 30.9s\n",
      "294:\tlearn: 0.1428105\ttotal: 12.9s\tremaining: 30.8s\n",
      "295:\tlearn: 0.1427196\ttotal: 12.9s\tremaining: 30.8s\n",
      "296:\tlearn: 0.1426300\ttotal: 13s\tremaining: 30.7s\n",
      "297:\tlearn: 0.1425132\ttotal: 13s\tremaining: 30.7s\n",
      "298:\tlearn: 0.1424009\ttotal: 13.1s\tremaining: 30.7s\n",
      "299:\tlearn: 0.1422578\ttotal: 13.1s\tremaining: 30.6s\n",
      "300:\tlearn: 0.1421327\ttotal: 13.2s\tremaining: 30.6s\n",
      "301:\tlearn: 0.1419985\ttotal: 13.2s\tremaining: 30.6s\n",
      "302:\tlearn: 0.1418554\ttotal: 13.3s\tremaining: 30.5s\n",
      "303:\tlearn: 0.1417383\ttotal: 13.3s\tremaining: 30.5s\n",
      "304:\tlearn: 0.1416480\ttotal: 13.4s\tremaining: 30.4s\n",
      "305:\tlearn: 0.1415452\ttotal: 13.4s\tremaining: 30.4s\n",
      "306:\tlearn: 0.1413450\ttotal: 13.4s\tremaining: 30.3s\n",
      "307:\tlearn: 0.1412296\ttotal: 13.5s\tremaining: 30.3s\n",
      "308:\tlearn: 0.1411264\ttotal: 13.5s\tremaining: 30.3s\n",
      "309:\tlearn: 0.1410029\ttotal: 13.6s\tremaining: 30.2s\n",
      "310:\tlearn: 0.1408955\ttotal: 13.6s\tremaining: 30.2s\n",
      "311:\tlearn: 0.1407076\ttotal: 13.7s\tremaining: 30.1s\n",
      "312:\tlearn: 0.1405475\ttotal: 13.7s\tremaining: 30.1s\n",
      "313:\tlearn: 0.1404233\ttotal: 13.7s\tremaining: 30s\n",
      "314:\tlearn: 0.1403845\ttotal: 13.8s\tremaining: 30s\n",
      "315:\tlearn: 0.1402472\ttotal: 13.8s\tremaining: 29.9s\n",
      "316:\tlearn: 0.1401221\ttotal: 13.9s\tremaining: 29.9s\n",
      "317:\tlearn: 0.1399384\ttotal: 13.9s\tremaining: 29.9s\n",
      "318:\tlearn: 0.1398504\ttotal: 14s\tremaining: 29.8s\n",
      "319:\tlearn: 0.1397312\ttotal: 14s\tremaining: 29.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320:\tlearn: 0.1395810\ttotal: 14.1s\tremaining: 29.7s\n",
      "321:\tlearn: 0.1394723\ttotal: 14.1s\tremaining: 29.7s\n",
      "322:\tlearn: 0.1392907\ttotal: 14.1s\tremaining: 29.6s\n",
      "323:\tlearn: 0.1391618\ttotal: 14.2s\tremaining: 29.6s\n",
      "324:\tlearn: 0.1390638\ttotal: 14.2s\tremaining: 29.5s\n",
      "325:\tlearn: 0.1389368\ttotal: 14.3s\tremaining: 29.5s\n",
      "326:\tlearn: 0.1388065\ttotal: 14.3s\tremaining: 29.5s\n",
      "327:\tlearn: 0.1386615\ttotal: 14.4s\tremaining: 29.4s\n",
      "328:\tlearn: 0.1384909\ttotal: 14.4s\tremaining: 29.4s\n",
      "329:\tlearn: 0.1383737\ttotal: 14.5s\tremaining: 29.4s\n",
      "330:\tlearn: 0.1382090\ttotal: 14.5s\tremaining: 29.3s\n",
      "331:\tlearn: 0.1380706\ttotal: 14.6s\tremaining: 29.3s\n",
      "332:\tlearn: 0.1380300\ttotal: 14.6s\tremaining: 29.2s\n",
      "333:\tlearn: 0.1379150\ttotal: 14.6s\tremaining: 29.2s\n",
      "334:\tlearn: 0.1378232\ttotal: 14.7s\tremaining: 29.1s\n",
      "335:\tlearn: 0.1376515\ttotal: 14.7s\tremaining: 29.1s\n",
      "336:\tlearn: 0.1375472\ttotal: 14.8s\tremaining: 29.1s\n",
      "337:\tlearn: 0.1375049\ttotal: 14.8s\tremaining: 29s\n",
      "338:\tlearn: 0.1373843\ttotal: 14.8s\tremaining: 29s\n",
      "339:\tlearn: 0.1372803\ttotal: 14.9s\tremaining: 28.9s\n",
      "340:\tlearn: 0.1371635\ttotal: 14.9s\tremaining: 28.9s\n",
      "341:\tlearn: 0.1370236\ttotal: 15s\tremaining: 28.8s\n",
      "342:\tlearn: 0.1368982\ttotal: 15s\tremaining: 28.8s\n",
      "343:\tlearn: 0.1367914\ttotal: 15.1s\tremaining: 28.7s\n",
      "344:\tlearn: 0.1366646\ttotal: 15.1s\tremaining: 28.7s\n",
      "345:\tlearn: 0.1365512\ttotal: 15.1s\tremaining: 28.6s\n",
      "346:\tlearn: 0.1364507\ttotal: 15.2s\tremaining: 28.6s\n",
      "347:\tlearn: 0.1363456\ttotal: 15.2s\tremaining: 28.5s\n",
      "348:\tlearn: 0.1362893\ttotal: 15.3s\tremaining: 28.5s\n",
      "349:\tlearn: 0.1361784\ttotal: 15.3s\tremaining: 28.4s\n",
      "350:\tlearn: 0.1360440\ttotal: 15.3s\tremaining: 28.4s\n",
      "351:\tlearn: 0.1359496\ttotal: 15.4s\tremaining: 28.3s\n",
      "352:\tlearn: 0.1358399\ttotal: 15.4s\tremaining: 28.3s\n",
      "353:\tlearn: 0.1356973\ttotal: 15.5s\tremaining: 28.2s\n",
      "354:\tlearn: 0.1356199\ttotal: 15.5s\tremaining: 28.2s\n",
      "355:\tlearn: 0.1354654\ttotal: 15.5s\tremaining: 28.1s\n",
      "356:\tlearn: 0.1353540\ttotal: 15.6s\tremaining: 28.1s\n",
      "357:\tlearn: 0.1352268\ttotal: 15.6s\tremaining: 28s\n",
      "358:\tlearn: 0.1351459\ttotal: 15.7s\tremaining: 28s\n",
      "359:\tlearn: 0.1349684\ttotal: 15.7s\tremaining: 27.9s\n",
      "360:\tlearn: 0.1348813\ttotal: 15.7s\tremaining: 27.9s\n",
      "361:\tlearn: 0.1347862\ttotal: 15.8s\tremaining: 27.8s\n",
      "362:\tlearn: 0.1346909\ttotal: 15.8s\tremaining: 27.8s\n",
      "363:\tlearn: 0.1345745\ttotal: 15.9s\tremaining: 27.7s\n",
      "364:\tlearn: 0.1344920\ttotal: 15.9s\tremaining: 27.7s\n",
      "365:\tlearn: 0.1343525\ttotal: 16s\tremaining: 27.6s\n",
      "366:\tlearn: 0.1342188\ttotal: 16s\tremaining: 27.6s\n",
      "367:\tlearn: 0.1340743\ttotal: 16s\tremaining: 27.6s\n",
      "368:\tlearn: 0.1339843\ttotal: 16.1s\tremaining: 27.5s\n",
      "369:\tlearn: 0.1338877\ttotal: 16.1s\tremaining: 27.5s\n",
      "370:\tlearn: 0.1338123\ttotal: 16.2s\tremaining: 27.4s\n",
      "371:\tlearn: 0.1337484\ttotal: 16.2s\tremaining: 27.4s\n",
      "372:\tlearn: 0.1336531\ttotal: 16.2s\tremaining: 27.3s\n",
      "373:\tlearn: 0.1335651\ttotal: 16.3s\tremaining: 27.3s\n",
      "374:\tlearn: 0.1334897\ttotal: 16.3s\tremaining: 27.2s\n",
      "375:\tlearn: 0.1333607\ttotal: 16.4s\tremaining: 27.2s\n",
      "376:\tlearn: 0.1332310\ttotal: 16.4s\tremaining: 27.1s\n",
      "377:\tlearn: 0.1331152\ttotal: 16.5s\tremaining: 27.1s\n",
      "378:\tlearn: 0.1330095\ttotal: 16.5s\tremaining: 27.1s\n",
      "379:\tlearn: 0.1329299\ttotal: 16.6s\tremaining: 27s\n",
      "380:\tlearn: 0.1327881\ttotal: 16.6s\tremaining: 27s\n",
      "381:\tlearn: 0.1326784\ttotal: 16.6s\tremaining: 26.9s\n",
      "382:\tlearn: 0.1325802\ttotal: 16.7s\tremaining: 26.9s\n",
      "383:\tlearn: 0.1324890\ttotal: 16.7s\tremaining: 26.8s\n",
      "384:\tlearn: 0.1323717\ttotal: 16.8s\tremaining: 26.8s\n",
      "385:\tlearn: 0.1322859\ttotal: 16.8s\tremaining: 26.8s\n",
      "386:\tlearn: 0.1321529\ttotal: 16.9s\tremaining: 26.7s\n",
      "387:\tlearn: 0.1320395\ttotal: 16.9s\tremaining: 26.7s\n",
      "388:\tlearn: 0.1319396\ttotal: 17s\tremaining: 26.6s\n",
      "389:\tlearn: 0.1318610\ttotal: 17s\tremaining: 26.6s\n",
      "390:\tlearn: 0.1317391\ttotal: 17s\tremaining: 26.5s\n",
      "391:\tlearn: 0.1316305\ttotal: 17.1s\tremaining: 26.5s\n",
      "392:\tlearn: 0.1315725\ttotal: 17.1s\tremaining: 26.5s\n",
      "393:\tlearn: 0.1315089\ttotal: 17.2s\tremaining: 26.4s\n",
      "394:\tlearn: 0.1314406\ttotal: 17.2s\tremaining: 26.4s\n",
      "395:\tlearn: 0.1313480\ttotal: 17.3s\tremaining: 26.3s\n",
      "396:\tlearn: 0.1312477\ttotal: 17.3s\tremaining: 26.3s\n",
      "397:\tlearn: 0.1311622\ttotal: 17.3s\tremaining: 26.2s\n",
      "398:\tlearn: 0.1310574\ttotal: 17.4s\tremaining: 26.2s\n",
      "399:\tlearn: 0.1309774\ttotal: 17.4s\tremaining: 26.1s\n",
      "400:\tlearn: 0.1309346\ttotal: 17.5s\tremaining: 26.1s\n",
      "401:\tlearn: 0.1308322\ttotal: 17.5s\tremaining: 26s\n",
      "402:\tlearn: 0.1307257\ttotal: 17.5s\tremaining: 26s\n",
      "403:\tlearn: 0.1306324\ttotal: 17.6s\tremaining: 25.9s\n",
      "404:\tlearn: 0.1305543\ttotal: 17.6s\tremaining: 25.9s\n",
      "405:\tlearn: 0.1304869\ttotal: 17.7s\tremaining: 25.8s\n",
      "406:\tlearn: 0.1303948\ttotal: 17.7s\tremaining: 25.8s\n",
      "407:\tlearn: 0.1302740\ttotal: 17.8s\tremaining: 25.8s\n",
      "408:\tlearn: 0.1301639\ttotal: 17.8s\tremaining: 25.7s\n",
      "409:\tlearn: 0.1300963\ttotal: 17.8s\tremaining: 25.6s\n",
      "410:\tlearn: 0.1300288\ttotal: 17.9s\tremaining: 25.6s\n",
      "411:\tlearn: 0.1299633\ttotal: 17.9s\tremaining: 25.6s\n",
      "412:\tlearn: 0.1298492\ttotal: 17.9s\tremaining: 25.5s\n",
      "413:\tlearn: 0.1297518\ttotal: 18s\tremaining: 25.5s\n",
      "414:\tlearn: 0.1296328\ttotal: 18s\tremaining: 25.4s\n",
      "415:\tlearn: 0.1295353\ttotal: 18.1s\tremaining: 25.4s\n",
      "416:\tlearn: 0.1294825\ttotal: 18.1s\tremaining: 25.3s\n",
      "417:\tlearn: 0.1294266\ttotal: 18.2s\tremaining: 25.3s\n",
      "418:\tlearn: 0.1293647\ttotal: 18.2s\tremaining: 25.3s\n",
      "419:\tlearn: 0.1293045\ttotal: 18.3s\tremaining: 25.2s\n",
      "420:\tlearn: 0.1292210\ttotal: 18.3s\tremaining: 25.2s\n",
      "421:\tlearn: 0.1291252\ttotal: 18.3s\tremaining: 25.1s\n",
      "422:\tlearn: 0.1290781\ttotal: 18.4s\tremaining: 25.1s\n",
      "423:\tlearn: 0.1289648\ttotal: 18.4s\tremaining: 25s\n",
      "424:\tlearn: 0.1289044\ttotal: 18.5s\tremaining: 25s\n",
      "425:\tlearn: 0.1288065\ttotal: 18.5s\tremaining: 24.9s\n",
      "426:\tlearn: 0.1286822\ttotal: 18.6s\tremaining: 24.9s\n",
      "427:\tlearn: 0.1285879\ttotal: 18.6s\tremaining: 24.8s\n",
      "428:\tlearn: 0.1284751\ttotal: 18.6s\tremaining: 24.8s\n",
      "429:\tlearn: 0.1283946\ttotal: 18.7s\tremaining: 24.8s\n",
      "430:\tlearn: 0.1282812\ttotal: 18.7s\tremaining: 24.7s\n",
      "431:\tlearn: 0.1281867\ttotal: 18.8s\tremaining: 24.7s\n",
      "432:\tlearn: 0.1281271\ttotal: 18.8s\tremaining: 24.6s\n",
      "433:\tlearn: 0.1280374\ttotal: 18.9s\tremaining: 24.6s\n",
      "434:\tlearn: 0.1279667\ttotal: 18.9s\tremaining: 24.5s\n",
      "435:\tlearn: 0.1278897\ttotal: 18.9s\tremaining: 24.5s\n",
      "436:\tlearn: 0.1277655\ttotal: 19s\tremaining: 24.5s\n",
      "437:\tlearn: 0.1276614\ttotal: 19s\tremaining: 24.4s\n",
      "438:\tlearn: 0.1275752\ttotal: 19.1s\tremaining: 24.4s\n",
      "439:\tlearn: 0.1274680\ttotal: 19.1s\tremaining: 24.3s\n",
      "440:\tlearn: 0.1273761\ttotal: 19.2s\tremaining: 24.3s\n",
      "441:\tlearn: 0.1273038\ttotal: 19.2s\tremaining: 24.3s\n",
      "442:\tlearn: 0.1272276\ttotal: 19.3s\tremaining: 24.2s\n",
      "443:\tlearn: 0.1271779\ttotal: 19.3s\tremaining: 24.2s\n",
      "444:\tlearn: 0.1271173\ttotal: 19.3s\tremaining: 24.1s\n",
      "445:\tlearn: 0.1270583\ttotal: 19.4s\tremaining: 24.1s\n",
      "446:\tlearn: 0.1269519\ttotal: 19.5s\tremaining: 24.1s\n",
      "447:\tlearn: 0.1268882\ttotal: 19.5s\tremaining: 24s\n",
      "448:\tlearn: 0.1268077\ttotal: 19.5s\tremaining: 24s\n",
      "449:\tlearn: 0.1267085\ttotal: 19.6s\tremaining: 23.9s\n",
      "450:\tlearn: 0.1266391\ttotal: 19.6s\tremaining: 23.9s\n",
      "451:\tlearn: 0.1265728\ttotal: 19.7s\tremaining: 23.8s\n",
      "452:\tlearn: 0.1264675\ttotal: 19.7s\tremaining: 23.8s\n",
      "453:\tlearn: 0.1263848\ttotal: 19.8s\tremaining: 23.8s\n",
      "454:\tlearn: 0.1262755\ttotal: 19.8s\tremaining: 23.7s\n",
      "455:\tlearn: 0.1262095\ttotal: 19.9s\tremaining: 23.7s\n",
      "456:\tlearn: 0.1261332\ttotal: 19.9s\tremaining: 23.6s\n",
      "457:\tlearn: 0.1260165\ttotal: 19.9s\tremaining: 23.6s\n",
      "458:\tlearn: 0.1259295\ttotal: 20s\tremaining: 23.6s\n",
      "459:\tlearn: 0.1258431\ttotal: 20s\tremaining: 23.5s\n",
      "460:\tlearn: 0.1257552\ttotal: 20.1s\tremaining: 23.5s\n",
      "461:\tlearn: 0.1256888\ttotal: 20.1s\tremaining: 23.4s\n",
      "462:\tlearn: 0.1256343\ttotal: 20.2s\tremaining: 23.4s\n",
      "463:\tlearn: 0.1255750\ttotal: 20.2s\tremaining: 23.3s\n",
      "464:\tlearn: 0.1255177\ttotal: 20.2s\tremaining: 23.3s\n",
      "465:\tlearn: 0.1254700\ttotal: 20.3s\tremaining: 23.2s\n",
      "466:\tlearn: 0.1253681\ttotal: 20.3s\tremaining: 23.2s\n",
      "467:\tlearn: 0.1252925\ttotal: 20.4s\tremaining: 23.1s\n",
      "468:\tlearn: 0.1252474\ttotal: 20.4s\tremaining: 23.1s\n",
      "469:\tlearn: 0.1251486\ttotal: 20.5s\tremaining: 23.1s\n",
      "470:\tlearn: 0.1251082\ttotal: 20.5s\tremaining: 23s\n",
      "471:\tlearn: 0.1250341\ttotal: 20.5s\tremaining: 23s\n",
      "472:\tlearn: 0.1249647\ttotal: 20.6s\tremaining: 22.9s\n",
      "473:\tlearn: 0.1248634\ttotal: 20.6s\tremaining: 22.9s\n",
      "474:\tlearn: 0.1247838\ttotal: 20.7s\tremaining: 22.8s\n",
      "475:\tlearn: 0.1246499\ttotal: 20.7s\tremaining: 22.8s\n",
      "476:\tlearn: 0.1245702\ttotal: 20.8s\tremaining: 22.8s\n",
      "477:\tlearn: 0.1244998\ttotal: 20.8s\tremaining: 22.7s\n",
      "478:\tlearn: 0.1244388\ttotal: 20.8s\tremaining: 22.7s\n",
      "479:\tlearn: 0.1243519\ttotal: 20.9s\tremaining: 22.6s\n",
      "480:\tlearn: 0.1242535\ttotal: 20.9s\tremaining: 22.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481:\tlearn: 0.1241807\ttotal: 21s\tremaining: 22.5s\n",
      "482:\tlearn: 0.1241245\ttotal: 21s\tremaining: 22.5s\n",
      "483:\tlearn: 0.1240174\ttotal: 21.1s\tremaining: 22.4s\n",
      "484:\tlearn: 0.1239635\ttotal: 21.1s\tremaining: 22.4s\n",
      "485:\tlearn: 0.1239140\ttotal: 21.1s\tremaining: 22.4s\n",
      "486:\tlearn: 0.1238387\ttotal: 21.2s\tremaining: 22.3s\n",
      "487:\tlearn: 0.1237321\ttotal: 21.2s\tremaining: 22.3s\n",
      "488:\tlearn: 0.1236581\ttotal: 21.3s\tremaining: 22.2s\n",
      "489:\tlearn: 0.1236120\ttotal: 21.3s\tremaining: 22.2s\n",
      "490:\tlearn: 0.1235466\ttotal: 21.4s\tremaining: 22.1s\n",
      "491:\tlearn: 0.1234707\ttotal: 21.4s\tremaining: 22.1s\n",
      "492:\tlearn: 0.1233779\ttotal: 21.4s\tremaining: 22.1s\n",
      "493:\tlearn: 0.1232917\ttotal: 21.5s\tremaining: 22s\n",
      "494:\tlearn: 0.1232475\ttotal: 21.5s\tremaining: 22s\n",
      "495:\tlearn: 0.1232049\ttotal: 21.6s\tremaining: 21.9s\n",
      "496:\tlearn: 0.1231361\ttotal: 21.6s\tremaining: 21.9s\n",
      "497:\tlearn: 0.1230431\ttotal: 21.7s\tremaining: 21.9s\n",
      "498:\tlearn: 0.1229015\ttotal: 21.7s\tremaining: 21.8s\n",
      "499:\tlearn: 0.1228202\ttotal: 21.8s\tremaining: 21.8s\n",
      "500:\tlearn: 0.1227679\ttotal: 21.8s\tremaining: 21.7s\n",
      "501:\tlearn: 0.1226866\ttotal: 21.9s\tremaining: 21.7s\n",
      "502:\tlearn: 0.1226150\ttotal: 21.9s\tremaining: 21.6s\n",
      "503:\tlearn: 0.1225108\ttotal: 21.9s\tremaining: 21.6s\n",
      "504:\tlearn: 0.1224292\ttotal: 22s\tremaining: 21.6s\n",
      "505:\tlearn: 0.1223238\ttotal: 22.1s\tremaining: 21.5s\n",
      "506:\tlearn: 0.1222370\ttotal: 22.1s\tremaining: 21.5s\n",
      "507:\tlearn: 0.1221487\ttotal: 22.1s\tremaining: 21.4s\n",
      "508:\tlearn: 0.1220839\ttotal: 22.2s\tremaining: 21.4s\n",
      "509:\tlearn: 0.1220327\ttotal: 22.2s\tremaining: 21.4s\n",
      "510:\tlearn: 0.1219608\ttotal: 22.3s\tremaining: 21.3s\n",
      "511:\tlearn: 0.1218888\ttotal: 22.3s\tremaining: 21.3s\n",
      "512:\tlearn: 0.1218316\ttotal: 22.4s\tremaining: 21.2s\n",
      "513:\tlearn: 0.1217435\ttotal: 22.4s\tremaining: 21.2s\n",
      "514:\tlearn: 0.1216982\ttotal: 22.4s\tremaining: 21.1s\n",
      "515:\tlearn: 0.1216384\ttotal: 22.5s\tremaining: 21.1s\n",
      "516:\tlearn: 0.1215949\ttotal: 22.5s\tremaining: 21s\n",
      "517:\tlearn: 0.1215047\ttotal: 22.6s\tremaining: 21s\n",
      "518:\tlearn: 0.1214049\ttotal: 22.6s\tremaining: 20.9s\n",
      "519:\tlearn: 0.1213158\ttotal: 22.7s\tremaining: 20.9s\n",
      "520:\tlearn: 0.1212396\ttotal: 22.7s\tremaining: 20.9s\n",
      "521:\tlearn: 0.1211580\ttotal: 22.8s\tremaining: 20.9s\n",
      "522:\tlearn: 0.1210693\ttotal: 22.8s\tremaining: 20.8s\n",
      "523:\tlearn: 0.1209863\ttotal: 22.9s\tremaining: 20.8s\n",
      "524:\tlearn: 0.1208737\ttotal: 23s\tremaining: 20.8s\n",
      "525:\tlearn: 0.1208172\ttotal: 23s\tremaining: 20.7s\n",
      "526:\tlearn: 0.1207600\ttotal: 23.1s\tremaining: 20.7s\n",
      "527:\tlearn: 0.1206880\ttotal: 23.2s\tremaining: 20.7s\n",
      "528:\tlearn: 0.1206011\ttotal: 23.2s\tremaining: 20.7s\n",
      "529:\tlearn: 0.1205419\ttotal: 23.3s\tremaining: 20.6s\n",
      "530:\tlearn: 0.1204755\ttotal: 23.3s\tremaining: 20.6s\n",
      "531:\tlearn: 0.1204104\ttotal: 23.4s\tremaining: 20.6s\n",
      "532:\tlearn: 0.1203149\ttotal: 23.4s\tremaining: 20.5s\n",
      "533:\tlearn: 0.1202303\ttotal: 23.5s\tremaining: 20.5s\n",
      "534:\tlearn: 0.1201456\ttotal: 23.5s\tremaining: 20.4s\n",
      "535:\tlearn: 0.1200537\ttotal: 23.6s\tremaining: 20.4s\n",
      "536:\tlearn: 0.1199791\ttotal: 23.6s\tremaining: 20.4s\n",
      "537:\tlearn: 0.1199331\ttotal: 23.7s\tremaining: 20.3s\n",
      "538:\tlearn: 0.1198475\ttotal: 23.7s\tremaining: 20.3s\n",
      "539:\tlearn: 0.1197882\ttotal: 23.8s\tremaining: 20.2s\n",
      "540:\tlearn: 0.1196877\ttotal: 23.8s\tremaining: 20.2s\n",
      "541:\tlearn: 0.1196236\ttotal: 23.8s\tremaining: 20.1s\n",
      "542:\tlearn: 0.1195689\ttotal: 23.9s\tremaining: 20.1s\n",
      "543:\tlearn: 0.1195289\ttotal: 23.9s\tremaining: 20.1s\n",
      "544:\tlearn: 0.1194686\ttotal: 24s\tremaining: 20s\n",
      "545:\tlearn: 0.1193924\ttotal: 24s\tremaining: 20s\n",
      "546:\tlearn: 0.1193107\ttotal: 24.1s\tremaining: 19.9s\n",
      "547:\tlearn: 0.1192596\ttotal: 24.1s\tremaining: 19.9s\n",
      "548:\tlearn: 0.1191965\ttotal: 24.1s\tremaining: 19.8s\n",
      "549:\tlearn: 0.1191233\ttotal: 24.2s\tremaining: 19.8s\n",
      "550:\tlearn: 0.1190329\ttotal: 24.2s\tremaining: 19.7s\n",
      "551:\tlearn: 0.1189661\ttotal: 24.3s\tremaining: 19.7s\n",
      "552:\tlearn: 0.1188785\ttotal: 24.3s\tremaining: 19.7s\n",
      "553:\tlearn: 0.1187980\ttotal: 24.4s\tremaining: 19.6s\n",
      "554:\tlearn: 0.1187533\ttotal: 24.4s\tremaining: 19.6s\n",
      "555:\tlearn: 0.1186650\ttotal: 24.5s\tremaining: 19.6s\n",
      "556:\tlearn: 0.1185911\ttotal: 24.5s\tremaining: 19.5s\n",
      "557:\tlearn: 0.1185545\ttotal: 24.6s\tremaining: 19.5s\n",
      "558:\tlearn: 0.1185137\ttotal: 24.6s\tremaining: 19.4s\n",
      "559:\tlearn: 0.1184626\ttotal: 24.7s\tremaining: 19.4s\n",
      "560:\tlearn: 0.1184173\ttotal: 24.7s\tremaining: 19.3s\n",
      "561:\tlearn: 0.1183677\ttotal: 24.7s\tremaining: 19.3s\n",
      "562:\tlearn: 0.1183351\ttotal: 24.8s\tremaining: 19.2s\n",
      "563:\tlearn: 0.1182811\ttotal: 24.8s\tremaining: 19.2s\n",
      "564:\tlearn: 0.1182237\ttotal: 24.9s\tremaining: 19.1s\n",
      "565:\tlearn: 0.1181836\ttotal: 24.9s\tremaining: 19.1s\n",
      "566:\tlearn: 0.1181249\ttotal: 24.9s\tremaining: 19s\n",
      "567:\tlearn: 0.1180659\ttotal: 25s\tremaining: 19s\n",
      "568:\tlearn: 0.1179834\ttotal: 25s\tremaining: 19s\n",
      "569:\tlearn: 0.1179273\ttotal: 25.1s\tremaining: 18.9s\n",
      "570:\tlearn: 0.1178762\ttotal: 25.1s\tremaining: 18.9s\n",
      "571:\tlearn: 0.1177956\ttotal: 25.2s\tremaining: 18.8s\n",
      "572:\tlearn: 0.1177592\ttotal: 25.2s\tremaining: 18.8s\n",
      "573:\tlearn: 0.1177059\ttotal: 25.2s\tremaining: 18.7s\n",
      "574:\tlearn: 0.1176196\ttotal: 25.3s\tremaining: 18.7s\n",
      "575:\tlearn: 0.1175432\ttotal: 25.3s\tremaining: 18.6s\n",
      "576:\tlearn: 0.1174780\ttotal: 25.4s\tremaining: 18.6s\n",
      "577:\tlearn: 0.1173908\ttotal: 25.4s\tremaining: 18.6s\n",
      "578:\tlearn: 0.1173068\ttotal: 25.5s\tremaining: 18.5s\n",
      "579:\tlearn: 0.1172730\ttotal: 25.5s\tremaining: 18.5s\n",
      "580:\tlearn: 0.1172171\ttotal: 25.5s\tremaining: 18.4s\n",
      "581:\tlearn: 0.1171371\ttotal: 25.6s\tremaining: 18.4s\n",
      "582:\tlearn: 0.1170767\ttotal: 25.6s\tremaining: 18.3s\n",
      "583:\tlearn: 0.1170097\ttotal: 25.7s\tremaining: 18.3s\n",
      "584:\tlearn: 0.1169385\ttotal: 25.7s\tremaining: 18.2s\n",
      "585:\tlearn: 0.1168956\ttotal: 25.8s\tremaining: 18.2s\n",
      "586:\tlearn: 0.1168277\ttotal: 25.8s\tremaining: 18.1s\n",
      "587:\tlearn: 0.1167265\ttotal: 25.8s\tremaining: 18.1s\n",
      "588:\tlearn: 0.1166754\ttotal: 25.9s\tremaining: 18.1s\n",
      "589:\tlearn: 0.1166033\ttotal: 25.9s\tremaining: 18s\n",
      "590:\tlearn: 0.1165485\ttotal: 26s\tremaining: 18s\n",
      "591:\tlearn: 0.1164728\ttotal: 26s\tremaining: 17.9s\n",
      "592:\tlearn: 0.1164268\ttotal: 26.1s\tremaining: 17.9s\n",
      "593:\tlearn: 0.1163687\ttotal: 26.1s\tremaining: 17.8s\n",
      "594:\tlearn: 0.1162944\ttotal: 26.1s\tremaining: 17.8s\n",
      "595:\tlearn: 0.1162431\ttotal: 26.2s\tremaining: 17.8s\n",
      "596:\tlearn: 0.1161888\ttotal: 26.2s\tremaining: 17.7s\n",
      "597:\tlearn: 0.1161054\ttotal: 26.3s\tremaining: 17.7s\n",
      "598:\tlearn: 0.1160129\ttotal: 26.3s\tremaining: 17.6s\n",
      "599:\tlearn: 0.1159685\ttotal: 26.4s\tremaining: 17.6s\n",
      "600:\tlearn: 0.1158943\ttotal: 26.4s\tremaining: 17.5s\n",
      "601:\tlearn: 0.1158141\ttotal: 26.4s\tremaining: 17.5s\n",
      "602:\tlearn: 0.1157288\ttotal: 26.5s\tremaining: 17.4s\n",
      "603:\tlearn: 0.1156590\ttotal: 26.5s\tremaining: 17.4s\n",
      "604:\tlearn: 0.1155791\ttotal: 26.6s\tremaining: 17.4s\n",
      "605:\tlearn: 0.1155283\ttotal: 26.6s\tremaining: 17.3s\n",
      "606:\tlearn: 0.1154863\ttotal: 26.7s\tremaining: 17.3s\n",
      "607:\tlearn: 0.1154287\ttotal: 26.7s\tremaining: 17.2s\n",
      "608:\tlearn: 0.1153568\ttotal: 26.8s\tremaining: 17.2s\n",
      "609:\tlearn: 0.1152919\ttotal: 26.8s\tremaining: 17.1s\n",
      "610:\tlearn: 0.1152507\ttotal: 26.8s\tremaining: 17.1s\n",
      "611:\tlearn: 0.1152086\ttotal: 26.9s\tremaining: 17s\n",
      "612:\tlearn: 0.1151419\ttotal: 26.9s\tremaining: 17s\n",
      "613:\tlearn: 0.1150786\ttotal: 27s\tremaining: 17s\n",
      "614:\tlearn: 0.1149950\ttotal: 27s\tremaining: 16.9s\n",
      "615:\tlearn: 0.1149272\ttotal: 27.1s\tremaining: 16.9s\n",
      "616:\tlearn: 0.1148721\ttotal: 27.1s\tremaining: 16.8s\n",
      "617:\tlearn: 0.1147957\ttotal: 27.2s\tremaining: 16.8s\n",
      "618:\tlearn: 0.1147393\ttotal: 27.2s\tremaining: 16.8s\n",
      "619:\tlearn: 0.1146776\ttotal: 27.3s\tremaining: 16.7s\n",
      "620:\tlearn: 0.1146496\ttotal: 27.3s\tremaining: 16.7s\n",
      "621:\tlearn: 0.1145503\ttotal: 27.4s\tremaining: 16.6s\n",
      "622:\tlearn: 0.1144957\ttotal: 27.4s\tremaining: 16.6s\n",
      "623:\tlearn: 0.1144488\ttotal: 27.5s\tremaining: 16.5s\n",
      "624:\tlearn: 0.1144069\ttotal: 27.5s\tremaining: 16.5s\n",
      "625:\tlearn: 0.1143574\ttotal: 27.5s\tremaining: 16.5s\n",
      "626:\tlearn: 0.1143206\ttotal: 27.6s\tremaining: 16.4s\n",
      "627:\tlearn: 0.1142825\ttotal: 27.6s\tremaining: 16.4s\n",
      "628:\tlearn: 0.1142077\ttotal: 27.7s\tremaining: 16.3s\n",
      "629:\tlearn: 0.1141399\ttotal: 27.8s\tremaining: 16.3s\n",
      "630:\tlearn: 0.1141054\ttotal: 27.8s\tremaining: 16.3s\n",
      "631:\tlearn: 0.1140426\ttotal: 27.9s\tremaining: 16.2s\n",
      "632:\tlearn: 0.1139535\ttotal: 27.9s\tremaining: 16.2s\n",
      "633:\tlearn: 0.1138884\ttotal: 28s\tremaining: 16.2s\n",
      "634:\tlearn: 0.1138189\ttotal: 28.1s\tremaining: 16.1s\n",
      "635:\tlearn: 0.1137646\ttotal: 28.1s\tremaining: 16.1s\n",
      "636:\tlearn: 0.1136884\ttotal: 28.2s\tremaining: 16.1s\n",
      "637:\tlearn: 0.1136512\ttotal: 28.2s\tremaining: 16s\n",
      "638:\tlearn: 0.1135956\ttotal: 28.3s\tremaining: 16s\n",
      "639:\tlearn: 0.1135353\ttotal: 28.3s\tremaining: 15.9s\n",
      "640:\tlearn: 0.1134957\ttotal: 28.4s\tremaining: 15.9s\n",
      "641:\tlearn: 0.1134676\ttotal: 28.5s\tremaining: 15.9s\n",
      "642:\tlearn: 0.1134221\ttotal: 28.5s\tremaining: 15.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643:\tlearn: 0.1133815\ttotal: 28.6s\tremaining: 15.8s\n",
      "644:\tlearn: 0.1132966\ttotal: 28.6s\tremaining: 15.7s\n",
      "645:\tlearn: 0.1132527\ttotal: 28.7s\tremaining: 15.7s\n",
      "646:\tlearn: 0.1131857\ttotal: 28.7s\tremaining: 15.7s\n",
      "647:\tlearn: 0.1131250\ttotal: 28.8s\tremaining: 15.6s\n",
      "648:\tlearn: 0.1130713\ttotal: 28.9s\tremaining: 15.6s\n",
      "649:\tlearn: 0.1130081\ttotal: 28.9s\tremaining: 15.6s\n",
      "650:\tlearn: 0.1129817\ttotal: 29s\tremaining: 15.5s\n",
      "651:\tlearn: 0.1129350\ttotal: 29.1s\tremaining: 15.5s\n",
      "652:\tlearn: 0.1128588\ttotal: 29.2s\tremaining: 15.5s\n",
      "653:\tlearn: 0.1128105\ttotal: 29.2s\tremaining: 15.5s\n",
      "654:\tlearn: 0.1127469\ttotal: 29.3s\tremaining: 15.4s\n",
      "655:\tlearn: 0.1127020\ttotal: 29.3s\tremaining: 15.4s\n",
      "656:\tlearn: 0.1126143\ttotal: 29.4s\tremaining: 15.4s\n",
      "657:\tlearn: 0.1125513\ttotal: 29.5s\tremaining: 15.3s\n",
      "658:\tlearn: 0.1124958\ttotal: 29.5s\tremaining: 15.3s\n",
      "659:\tlearn: 0.1124232\ttotal: 29.6s\tremaining: 15.2s\n",
      "660:\tlearn: 0.1123616\ttotal: 29.7s\tremaining: 15.2s\n",
      "661:\tlearn: 0.1123177\ttotal: 29.7s\tremaining: 15.2s\n",
      "662:\tlearn: 0.1122700\ttotal: 29.8s\tremaining: 15.1s\n",
      "663:\tlearn: 0.1122120\ttotal: 29.8s\tremaining: 15.1s\n",
      "664:\tlearn: 0.1121646\ttotal: 29.9s\tremaining: 15.1s\n",
      "665:\tlearn: 0.1120860\ttotal: 29.9s\tremaining: 15s\n",
      "666:\tlearn: 0.1120217\ttotal: 30s\tremaining: 15s\n",
      "667:\tlearn: 0.1119623\ttotal: 30.1s\tremaining: 14.9s\n",
      "668:\tlearn: 0.1118742\ttotal: 30.1s\tremaining: 14.9s\n",
      "669:\tlearn: 0.1117813\ttotal: 30.2s\tremaining: 14.9s\n",
      "670:\tlearn: 0.1117157\ttotal: 30.2s\tremaining: 14.8s\n",
      "671:\tlearn: 0.1116912\ttotal: 30.3s\tremaining: 14.8s\n",
      "672:\tlearn: 0.1116564\ttotal: 30.3s\tremaining: 14.7s\n",
      "673:\tlearn: 0.1116172\ttotal: 30.4s\tremaining: 14.7s\n",
      "674:\tlearn: 0.1115293\ttotal: 30.4s\tremaining: 14.6s\n",
      "675:\tlearn: 0.1114818\ttotal: 30.5s\tremaining: 14.6s\n",
      "676:\tlearn: 0.1114167\ttotal: 30.5s\tremaining: 14.6s\n",
      "677:\tlearn: 0.1113807\ttotal: 30.6s\tremaining: 14.5s\n",
      "678:\tlearn: 0.1113229\ttotal: 30.7s\tremaining: 14.5s\n",
      "679:\tlearn: 0.1112866\ttotal: 30.7s\tremaining: 14.5s\n",
      "680:\tlearn: 0.1112348\ttotal: 30.8s\tremaining: 14.4s\n",
      "681:\tlearn: 0.1111930\ttotal: 30.8s\tremaining: 14.4s\n",
      "682:\tlearn: 0.1111390\ttotal: 30.9s\tremaining: 14.3s\n",
      "683:\tlearn: 0.1110738\ttotal: 31s\tremaining: 14.3s\n",
      "684:\tlearn: 0.1110455\ttotal: 31s\tremaining: 14.3s\n",
      "685:\tlearn: 0.1110139\ttotal: 31.1s\tremaining: 14.2s\n",
      "686:\tlearn: 0.1109768\ttotal: 31.1s\tremaining: 14.2s\n",
      "687:\tlearn: 0.1109316\ttotal: 31.2s\tremaining: 14.1s\n",
      "688:\tlearn: 0.1108590\ttotal: 31.2s\tremaining: 14.1s\n",
      "689:\tlearn: 0.1108089\ttotal: 31.3s\tremaining: 14.1s\n",
      "690:\tlearn: 0.1107664\ttotal: 31.3s\tremaining: 14s\n",
      "691:\tlearn: 0.1107122\ttotal: 31.4s\tremaining: 14s\n",
      "692:\tlearn: 0.1106610\ttotal: 31.4s\tremaining: 13.9s\n",
      "693:\tlearn: 0.1106085\ttotal: 31.5s\tremaining: 13.9s\n",
      "694:\tlearn: 0.1105219\ttotal: 31.6s\tremaining: 13.8s\n",
      "695:\tlearn: 0.1104741\ttotal: 31.6s\tremaining: 13.8s\n",
      "696:\tlearn: 0.1104253\ttotal: 31.7s\tremaining: 13.8s\n",
      "697:\tlearn: 0.1103534\ttotal: 31.7s\tremaining: 13.7s\n",
      "698:\tlearn: 0.1102741\ttotal: 31.8s\tremaining: 13.7s\n",
      "699:\tlearn: 0.1102170\ttotal: 31.8s\tremaining: 13.6s\n",
      "700:\tlearn: 0.1101779\ttotal: 31.9s\tremaining: 13.6s\n",
      "701:\tlearn: 0.1101418\ttotal: 32s\tremaining: 13.6s\n",
      "702:\tlearn: 0.1101038\ttotal: 32s\tremaining: 13.5s\n",
      "703:\tlearn: 0.1100480\ttotal: 32.1s\tremaining: 13.5s\n",
      "704:\tlearn: 0.1099957\ttotal: 32.1s\tremaining: 13.4s\n",
      "705:\tlearn: 0.1099279\ttotal: 32.2s\tremaining: 13.4s\n",
      "706:\tlearn: 0.1098912\ttotal: 32.2s\tremaining: 13.3s\n",
      "707:\tlearn: 0.1098531\ttotal: 32.3s\tremaining: 13.3s\n",
      "708:\tlearn: 0.1098018\ttotal: 32.3s\tremaining: 13.3s\n",
      "709:\tlearn: 0.1097516\ttotal: 32.4s\tremaining: 13.2s\n",
      "710:\tlearn: 0.1096898\ttotal: 32.4s\tremaining: 13.2s\n",
      "711:\tlearn: 0.1096680\ttotal: 32.4s\tremaining: 13.1s\n",
      "712:\tlearn: 0.1096157\ttotal: 32.5s\tremaining: 13.1s\n",
      "713:\tlearn: 0.1095535\ttotal: 32.5s\tremaining: 13s\n",
      "714:\tlearn: 0.1094923\ttotal: 32.6s\tremaining: 13s\n",
      "715:\tlearn: 0.1094281\ttotal: 32.6s\tremaining: 12.9s\n",
      "716:\tlearn: 0.1093589\ttotal: 32.7s\tremaining: 12.9s\n",
      "717:\tlearn: 0.1093093\ttotal: 32.8s\tremaining: 12.9s\n",
      "718:\tlearn: 0.1092470\ttotal: 32.8s\tremaining: 12.8s\n",
      "719:\tlearn: 0.1092007\ttotal: 32.9s\tremaining: 12.8s\n",
      "720:\tlearn: 0.1091464\ttotal: 32.9s\tremaining: 12.7s\n",
      "721:\tlearn: 0.1091071\ttotal: 33s\tremaining: 12.7s\n",
      "722:\tlearn: 0.1090525\ttotal: 33s\tremaining: 12.6s\n",
      "723:\tlearn: 0.1089737\ttotal: 33.1s\tremaining: 12.6s\n",
      "724:\tlearn: 0.1089334\ttotal: 33.1s\tremaining: 12.6s\n",
      "725:\tlearn: 0.1088733\ttotal: 33.2s\tremaining: 12.5s\n",
      "726:\tlearn: 0.1087903\ttotal: 33.2s\tremaining: 12.5s\n",
      "727:\tlearn: 0.1087234\ttotal: 33.3s\tremaining: 12.4s\n",
      "728:\tlearn: 0.1086994\ttotal: 33.3s\tremaining: 12.4s\n",
      "729:\tlearn: 0.1086793\ttotal: 33.3s\tremaining: 12.3s\n",
      "730:\tlearn: 0.1086355\ttotal: 33.4s\tremaining: 12.3s\n",
      "731:\tlearn: 0.1085804\ttotal: 33.4s\tremaining: 12.2s\n",
      "732:\tlearn: 0.1085179\ttotal: 33.5s\tremaining: 12.2s\n",
      "733:\tlearn: 0.1084741\ttotal: 33.5s\tremaining: 12.2s\n",
      "734:\tlearn: 0.1084156\ttotal: 33.6s\tremaining: 12.1s\n",
      "735:\tlearn: 0.1083365\ttotal: 33.6s\tremaining: 12.1s\n",
      "736:\tlearn: 0.1082781\ttotal: 33.7s\tremaining: 12s\n",
      "737:\tlearn: 0.1081962\ttotal: 33.8s\tremaining: 12s\n",
      "738:\tlearn: 0.1081521\ttotal: 33.8s\tremaining: 11.9s\n",
      "739:\tlearn: 0.1081185\ttotal: 33.9s\tremaining: 11.9s\n",
      "740:\tlearn: 0.1080823\ttotal: 33.9s\tremaining: 11.8s\n",
      "741:\tlearn: 0.1080548\ttotal: 33.9s\tremaining: 11.8s\n",
      "742:\tlearn: 0.1079997\ttotal: 34s\tremaining: 11.8s\n",
      "743:\tlearn: 0.1079401\ttotal: 34s\tremaining: 11.7s\n",
      "744:\tlearn: 0.1078660\ttotal: 34.1s\tremaining: 11.7s\n",
      "745:\tlearn: 0.1078476\ttotal: 34.1s\tremaining: 11.6s\n",
      "746:\tlearn: 0.1078081\ttotal: 34.2s\tremaining: 11.6s\n",
      "747:\tlearn: 0.1077852\ttotal: 34.2s\tremaining: 11.5s\n",
      "748:\tlearn: 0.1077507\ttotal: 34.3s\tremaining: 11.5s\n",
      "749:\tlearn: 0.1077248\ttotal: 34.3s\tremaining: 11.4s\n",
      "750:\tlearn: 0.1076855\ttotal: 34.3s\tremaining: 11.4s\n",
      "751:\tlearn: 0.1076220\ttotal: 34.4s\tremaining: 11.3s\n",
      "752:\tlearn: 0.1075477\ttotal: 34.4s\tremaining: 11.3s\n",
      "753:\tlearn: 0.1074767\ttotal: 34.5s\tremaining: 11.3s\n",
      "754:\tlearn: 0.1074502\ttotal: 34.6s\tremaining: 11.2s\n",
      "755:\tlearn: 0.1074086\ttotal: 34.6s\tremaining: 11.2s\n",
      "756:\tlearn: 0.1073674\ttotal: 34.7s\tremaining: 11.1s\n",
      "757:\tlearn: 0.1073399\ttotal: 34.7s\tremaining: 11.1s\n",
      "758:\tlearn: 0.1072763\ttotal: 34.8s\tremaining: 11.1s\n",
      "759:\tlearn: 0.1072429\ttotal: 34.9s\tremaining: 11s\n",
      "760:\tlearn: 0.1071693\ttotal: 34.9s\tremaining: 11s\n",
      "761:\tlearn: 0.1071112\ttotal: 35s\tremaining: 10.9s\n",
      "762:\tlearn: 0.1070528\ttotal: 35s\tremaining: 10.9s\n",
      "763:\tlearn: 0.1070293\ttotal: 35.1s\tremaining: 10.8s\n",
      "764:\tlearn: 0.1069819\ttotal: 35.1s\tremaining: 10.8s\n",
      "765:\tlearn: 0.1069223\ttotal: 35.2s\tremaining: 10.8s\n",
      "766:\tlearn: 0.1068927\ttotal: 35.3s\tremaining: 10.7s\n",
      "767:\tlearn: 0.1068529\ttotal: 35.3s\tremaining: 10.7s\n",
      "768:\tlearn: 0.1068129\ttotal: 35.4s\tremaining: 10.6s\n",
      "769:\tlearn: 0.1067900\ttotal: 35.4s\tremaining: 10.6s\n",
      "770:\tlearn: 0.1067058\ttotal: 35.5s\tremaining: 10.5s\n",
      "771:\tlearn: 0.1066682\ttotal: 35.5s\tremaining: 10.5s\n",
      "772:\tlearn: 0.1066100\ttotal: 35.6s\tremaining: 10.4s\n",
      "773:\tlearn: 0.1065614\ttotal: 35.7s\tremaining: 10.4s\n",
      "774:\tlearn: 0.1065268\ttotal: 35.7s\tremaining: 10.4s\n",
      "775:\tlearn: 0.1064747\ttotal: 35.8s\tremaining: 10.3s\n",
      "776:\tlearn: 0.1064037\ttotal: 35.8s\tremaining: 10.3s\n",
      "777:\tlearn: 0.1063475\ttotal: 35.9s\tremaining: 10.2s\n",
      "778:\tlearn: 0.1063215\ttotal: 35.9s\tremaining: 10.2s\n",
      "779:\tlearn: 0.1062475\ttotal: 36s\tremaining: 10.2s\n",
      "780:\tlearn: 0.1062112\ttotal: 36.1s\tremaining: 10.1s\n",
      "781:\tlearn: 0.1061497\ttotal: 36.1s\tremaining: 10.1s\n",
      "782:\tlearn: 0.1061249\ttotal: 36.2s\tremaining: 10s\n",
      "783:\tlearn: 0.1060773\ttotal: 36.2s\tremaining: 9.98s\n",
      "784:\tlearn: 0.1060268\ttotal: 36.3s\tremaining: 9.93s\n",
      "785:\tlearn: 0.1059603\ttotal: 36.3s\tremaining: 9.89s\n",
      "786:\tlearn: 0.1059198\ttotal: 36.4s\tremaining: 9.85s\n",
      "787:\tlearn: 0.1058637\ttotal: 36.4s\tremaining: 9.8s\n",
      "788:\tlearn: 0.1058130\ttotal: 36.5s\tremaining: 9.76s\n",
      "789:\tlearn: 0.1057844\ttotal: 36.5s\tremaining: 9.71s\n",
      "790:\tlearn: 0.1057303\ttotal: 36.6s\tremaining: 9.67s\n",
      "791:\tlearn: 0.1056615\ttotal: 36.7s\tremaining: 9.63s\n",
      "792:\tlearn: 0.1055973\ttotal: 36.7s\tremaining: 9.58s\n",
      "793:\tlearn: 0.1055347\ttotal: 36.8s\tremaining: 9.54s\n",
      "794:\tlearn: 0.1054910\ttotal: 36.8s\tremaining: 9.49s\n",
      "795:\tlearn: 0.1054597\ttotal: 36.9s\tremaining: 9.45s\n",
      "796:\tlearn: 0.1054179\ttotal: 36.9s\tremaining: 9.4s\n",
      "797:\tlearn: 0.1053397\ttotal: 37s\tremaining: 9.36s\n",
      "798:\tlearn: 0.1053090\ttotal: 37s\tremaining: 9.31s\n",
      "799:\tlearn: 0.1052637\ttotal: 37.1s\tremaining: 9.27s\n",
      "800:\tlearn: 0.1052269\ttotal: 37.1s\tremaining: 9.22s\n",
      "801:\tlearn: 0.1051650\ttotal: 37.2s\tremaining: 9.18s\n",
      "802:\tlearn: 0.1051220\ttotal: 37.2s\tremaining: 9.13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803:\tlearn: 0.1050881\ttotal: 37.3s\tremaining: 9.09s\n",
      "804:\tlearn: 0.1050450\ttotal: 37.3s\tremaining: 9.04s\n",
      "805:\tlearn: 0.1049987\ttotal: 37.4s\tremaining: 8.99s\n",
      "806:\tlearn: 0.1049472\ttotal: 37.4s\tremaining: 8.95s\n",
      "807:\tlearn: 0.1049156\ttotal: 37.5s\tremaining: 8.9s\n",
      "808:\tlearn: 0.1048593\ttotal: 37.5s\tremaining: 8.86s\n",
      "809:\tlearn: 0.1048009\ttotal: 37.6s\tremaining: 8.81s\n",
      "810:\tlearn: 0.1047451\ttotal: 37.6s\tremaining: 8.77s\n",
      "811:\tlearn: 0.1046977\ttotal: 37.7s\tremaining: 8.72s\n",
      "812:\tlearn: 0.1046237\ttotal: 37.7s\tremaining: 8.68s\n",
      "813:\tlearn: 0.1045652\ttotal: 37.8s\tremaining: 8.63s\n",
      "814:\tlearn: 0.1045044\ttotal: 37.8s\tremaining: 8.59s\n",
      "815:\tlearn: 0.1044690\ttotal: 37.9s\tremaining: 8.54s\n",
      "816:\tlearn: 0.1044147\ttotal: 37.9s\tremaining: 8.5s\n",
      "817:\tlearn: 0.1043736\ttotal: 38s\tremaining: 8.45s\n",
      "818:\tlearn: 0.1043426\ttotal: 38s\tremaining: 8.4s\n",
      "819:\tlearn: 0.1042999\ttotal: 38.1s\tremaining: 8.36s\n",
      "820:\tlearn: 0.1042751\ttotal: 38.1s\tremaining: 8.31s\n",
      "821:\tlearn: 0.1042187\ttotal: 38.2s\tremaining: 8.27s\n",
      "822:\tlearn: 0.1041597\ttotal: 38.2s\tremaining: 8.22s\n",
      "823:\tlearn: 0.1041355\ttotal: 38.3s\tremaining: 8.17s\n",
      "824:\tlearn: 0.1040948\ttotal: 38.3s\tremaining: 8.13s\n",
      "825:\tlearn: 0.1040623\ttotal: 38.4s\tremaining: 8.08s\n",
      "826:\tlearn: 0.1040066\ttotal: 38.4s\tremaining: 8.03s\n",
      "827:\tlearn: 0.1039610\ttotal: 38.5s\tremaining: 7.99s\n",
      "828:\tlearn: 0.1039306\ttotal: 38.5s\tremaining: 7.94s\n",
      "829:\tlearn: 0.1038610\ttotal: 38.6s\tremaining: 7.9s\n",
      "830:\tlearn: 0.1038009\ttotal: 38.6s\tremaining: 7.85s\n",
      "831:\tlearn: 0.1037443\ttotal: 38.7s\tremaining: 7.8s\n",
      "832:\tlearn: 0.1037044\ttotal: 38.7s\tremaining: 7.76s\n",
      "833:\tlearn: 0.1036798\ttotal: 38.7s\tremaining: 7.71s\n",
      "834:\tlearn: 0.1036440\ttotal: 38.8s\tremaining: 7.66s\n",
      "835:\tlearn: 0.1036125\ttotal: 38.8s\tremaining: 7.62s\n",
      "836:\tlearn: 0.1035694\ttotal: 38.9s\tremaining: 7.57s\n",
      "837:\tlearn: 0.1035241\ttotal: 38.9s\tremaining: 7.53s\n",
      "838:\tlearn: 0.1034867\ttotal: 39s\tremaining: 7.48s\n",
      "839:\tlearn: 0.1034309\ttotal: 39s\tremaining: 7.43s\n",
      "840:\tlearn: 0.1034058\ttotal: 39.1s\tremaining: 7.39s\n",
      "841:\tlearn: 0.1033707\ttotal: 39.1s\tremaining: 7.34s\n",
      "842:\tlearn: 0.1033292\ttotal: 39.2s\tremaining: 7.3s\n",
      "843:\tlearn: 0.1032897\ttotal: 39.2s\tremaining: 7.25s\n",
      "844:\tlearn: 0.1032588\ttotal: 39.3s\tremaining: 7.2s\n",
      "845:\tlearn: 0.1032244\ttotal: 39.3s\tremaining: 7.16s\n",
      "846:\tlearn: 0.1031718\ttotal: 39.4s\tremaining: 7.11s\n",
      "847:\tlearn: 0.1031312\ttotal: 39.4s\tremaining: 7.06s\n",
      "848:\tlearn: 0.1030895\ttotal: 39.5s\tremaining: 7.02s\n",
      "849:\tlearn: 0.1030495\ttotal: 39.5s\tremaining: 6.97s\n",
      "850:\tlearn: 0.1029911\ttotal: 39.6s\tremaining: 6.93s\n",
      "851:\tlearn: 0.1029608\ttotal: 39.6s\tremaining: 6.88s\n",
      "852:\tlearn: 0.1029125\ttotal: 39.7s\tremaining: 6.84s\n",
      "853:\tlearn: 0.1028798\ttotal: 39.7s\tremaining: 6.79s\n",
      "854:\tlearn: 0.1028325\ttotal: 39.8s\tremaining: 6.74s\n",
      "855:\tlearn: 0.1028013\ttotal: 39.8s\tremaining: 6.7s\n",
      "856:\tlearn: 0.1027430\ttotal: 39.9s\tremaining: 6.66s\n",
      "857:\tlearn: 0.1026883\ttotal: 39.9s\tremaining: 6.61s\n",
      "858:\tlearn: 0.1026557\ttotal: 40s\tremaining: 6.56s\n",
      "859:\tlearn: 0.1026152\ttotal: 40s\tremaining: 6.52s\n",
      "860:\tlearn: 0.1025511\ttotal: 40.1s\tremaining: 6.47s\n",
      "861:\tlearn: 0.1025209\ttotal: 40.1s\tremaining: 6.43s\n",
      "862:\tlearn: 0.1024600\ttotal: 40.2s\tremaining: 6.38s\n",
      "863:\tlearn: 0.1024110\ttotal: 40.2s\tremaining: 6.33s\n",
      "864:\tlearn: 0.1023600\ttotal: 40.3s\tremaining: 6.29s\n",
      "865:\tlearn: 0.1022937\ttotal: 40.3s\tremaining: 6.24s\n",
      "866:\tlearn: 0.1022519\ttotal: 40.4s\tremaining: 6.2s\n",
      "867:\tlearn: 0.1022174\ttotal: 40.4s\tremaining: 6.15s\n",
      "868:\tlearn: 0.1021672\ttotal: 40.5s\tremaining: 6.1s\n",
      "869:\tlearn: 0.1021397\ttotal: 40.5s\tremaining: 6.06s\n",
      "870:\tlearn: 0.1020904\ttotal: 40.6s\tremaining: 6.01s\n",
      "871:\tlearn: 0.1020464\ttotal: 40.6s\tremaining: 5.97s\n",
      "872:\tlearn: 0.1019995\ttotal: 40.7s\tremaining: 5.92s\n",
      "873:\tlearn: 0.1019528\ttotal: 40.8s\tremaining: 5.88s\n",
      "874:\tlearn: 0.1018866\ttotal: 40.8s\tremaining: 5.83s\n",
      "875:\tlearn: 0.1018391\ttotal: 40.9s\tremaining: 5.78s\n",
      "876:\tlearn: 0.1017972\ttotal: 40.9s\tremaining: 5.74s\n",
      "877:\tlearn: 0.1017682\ttotal: 41s\tremaining: 5.69s\n",
      "878:\tlearn: 0.1017458\ttotal: 41s\tremaining: 5.64s\n",
      "879:\tlearn: 0.1016946\ttotal: 41.1s\tremaining: 5.6s\n",
      "880:\tlearn: 0.1016604\ttotal: 41.1s\tremaining: 5.55s\n",
      "881:\tlearn: 0.1016140\ttotal: 41.1s\tremaining: 5.5s\n",
      "882:\tlearn: 0.1015533\ttotal: 41.2s\tremaining: 5.46s\n",
      "883:\tlearn: 0.1015199\ttotal: 41.2s\tremaining: 5.41s\n",
      "884:\tlearn: 0.1014714\ttotal: 41.3s\tremaining: 5.36s\n",
      "885:\tlearn: 0.1014219\ttotal: 41.3s\tremaining: 5.32s\n",
      "886:\tlearn: 0.1013852\ttotal: 41.4s\tremaining: 5.27s\n",
      "887:\tlearn: 0.1013397\ttotal: 41.4s\tremaining: 5.22s\n",
      "888:\tlearn: 0.1012989\ttotal: 41.5s\tremaining: 5.18s\n",
      "889:\tlearn: 0.1012691\ttotal: 41.5s\tremaining: 5.13s\n",
      "890:\tlearn: 0.1012191\ttotal: 41.6s\tremaining: 5.09s\n",
      "891:\tlearn: 0.1011859\ttotal: 41.6s\tremaining: 5.04s\n",
      "892:\tlearn: 0.1011379\ttotal: 41.7s\tremaining: 4.99s\n",
      "893:\tlearn: 0.1010979\ttotal: 41.7s\tremaining: 4.95s\n",
      "894:\tlearn: 0.1010711\ttotal: 41.8s\tremaining: 4.9s\n",
      "895:\tlearn: 0.1010418\ttotal: 41.8s\tremaining: 4.85s\n",
      "896:\tlearn: 0.1009986\ttotal: 41.9s\tremaining: 4.8s\n",
      "897:\tlearn: 0.1009654\ttotal: 41.9s\tremaining: 4.76s\n",
      "898:\tlearn: 0.1009161\ttotal: 41.9s\tremaining: 4.71s\n",
      "899:\tlearn: 0.1008642\ttotal: 42s\tremaining: 4.67s\n",
      "900:\tlearn: 0.1008258\ttotal: 42s\tremaining: 4.62s\n",
      "901:\tlearn: 0.1007822\ttotal: 42.1s\tremaining: 4.57s\n",
      "902:\tlearn: 0.1007535\ttotal: 42.1s\tremaining: 4.53s\n",
      "903:\tlearn: 0.1007237\ttotal: 42.2s\tremaining: 4.48s\n",
      "904:\tlearn: 0.1007052\ttotal: 42.2s\tremaining: 4.43s\n",
      "905:\tlearn: 0.1006569\ttotal: 42.3s\tremaining: 4.38s\n",
      "906:\tlearn: 0.1006185\ttotal: 42.3s\tremaining: 4.34s\n",
      "907:\tlearn: 0.1005733\ttotal: 42.4s\tremaining: 4.29s\n",
      "908:\tlearn: 0.1005283\ttotal: 42.4s\tremaining: 4.25s\n",
      "909:\tlearn: 0.1004786\ttotal: 42.5s\tremaining: 4.2s\n",
      "910:\tlearn: 0.1004382\ttotal: 42.5s\tremaining: 4.15s\n",
      "911:\tlearn: 0.1003878\ttotal: 42.6s\tremaining: 4.11s\n",
      "912:\tlearn: 0.1003555\ttotal: 42.6s\tremaining: 4.06s\n",
      "913:\tlearn: 0.1003380\ttotal: 42.7s\tremaining: 4.01s\n",
      "914:\tlearn: 0.1003159\ttotal: 42.7s\tremaining: 3.97s\n",
      "915:\tlearn: 0.1002702\ttotal: 42.8s\tremaining: 3.92s\n",
      "916:\tlearn: 0.1002157\ttotal: 42.8s\tremaining: 3.87s\n",
      "917:\tlearn: 0.1001868\ttotal: 42.9s\tremaining: 3.83s\n",
      "918:\tlearn: 0.1001320\ttotal: 42.9s\tremaining: 3.78s\n",
      "919:\tlearn: 0.1000956\ttotal: 43s\tremaining: 3.73s\n",
      "920:\tlearn: 0.1000380\ttotal: 43s\tremaining: 3.69s\n",
      "921:\tlearn: 0.1000043\ttotal: 43.1s\tremaining: 3.64s\n",
      "922:\tlearn: 0.0999668\ttotal: 43.1s\tremaining: 3.6s\n",
      "923:\tlearn: 0.0999327\ttotal: 43.1s\tremaining: 3.55s\n",
      "924:\tlearn: 0.0999069\ttotal: 43.2s\tremaining: 3.5s\n",
      "925:\tlearn: 0.0998766\ttotal: 43.2s\tremaining: 3.45s\n",
      "926:\tlearn: 0.0998364\ttotal: 43.3s\tremaining: 3.41s\n",
      "927:\tlearn: 0.0997945\ttotal: 43.3s\tremaining: 3.36s\n",
      "928:\tlearn: 0.0997436\ttotal: 43.4s\tremaining: 3.31s\n",
      "929:\tlearn: 0.0997113\ttotal: 43.4s\tremaining: 3.27s\n",
      "930:\tlearn: 0.0996722\ttotal: 43.5s\tremaining: 3.22s\n",
      "931:\tlearn: 0.0996358\ttotal: 43.5s\tremaining: 3.17s\n",
      "932:\tlearn: 0.0995876\ttotal: 43.6s\tremaining: 3.13s\n",
      "933:\tlearn: 0.0995608\ttotal: 43.6s\tremaining: 3.08s\n",
      "934:\tlearn: 0.0995375\ttotal: 43.6s\tremaining: 3.03s\n",
      "935:\tlearn: 0.0994784\ttotal: 43.7s\tremaining: 2.99s\n",
      "936:\tlearn: 0.0994404\ttotal: 43.7s\tremaining: 2.94s\n",
      "937:\tlearn: 0.0993974\ttotal: 43.8s\tremaining: 2.89s\n",
      "938:\tlearn: 0.0993350\ttotal: 43.8s\tremaining: 2.85s\n",
      "939:\tlearn: 0.0992892\ttotal: 43.9s\tremaining: 2.8s\n",
      "940:\tlearn: 0.0992598\ttotal: 43.9s\tremaining: 2.75s\n",
      "941:\tlearn: 0.0992306\ttotal: 44s\tremaining: 2.71s\n",
      "942:\tlearn: 0.0991889\ttotal: 44s\tremaining: 2.66s\n",
      "943:\tlearn: 0.0991566\ttotal: 44.1s\tremaining: 2.62s\n",
      "944:\tlearn: 0.0991079\ttotal: 44.1s\tremaining: 2.57s\n",
      "945:\tlearn: 0.0990737\ttotal: 44.2s\tremaining: 2.52s\n",
      "946:\tlearn: 0.0990315\ttotal: 44.2s\tremaining: 2.48s\n",
      "947:\tlearn: 0.0990014\ttotal: 44.3s\tremaining: 2.43s\n",
      "948:\tlearn: 0.0989791\ttotal: 44.3s\tremaining: 2.38s\n",
      "949:\tlearn: 0.0989596\ttotal: 44.4s\tremaining: 2.33s\n",
      "950:\tlearn: 0.0989344\ttotal: 44.4s\tremaining: 2.29s\n",
      "951:\tlearn: 0.0989063\ttotal: 44.4s\tremaining: 2.24s\n",
      "952:\tlearn: 0.0988778\ttotal: 44.5s\tremaining: 2.19s\n",
      "953:\tlearn: 0.0988130\ttotal: 44.5s\tremaining: 2.15s\n",
      "954:\tlearn: 0.0987602\ttotal: 44.6s\tremaining: 2.1s\n",
      "955:\tlearn: 0.0986922\ttotal: 44.6s\tremaining: 2.05s\n",
      "956:\tlearn: 0.0986615\ttotal: 44.7s\tremaining: 2.01s\n",
      "957:\tlearn: 0.0985939\ttotal: 44.7s\tremaining: 1.96s\n",
      "958:\tlearn: 0.0985550\ttotal: 44.8s\tremaining: 1.91s\n",
      "959:\tlearn: 0.0984974\ttotal: 44.8s\tremaining: 1.87s\n",
      "960:\tlearn: 0.0984565\ttotal: 44.9s\tremaining: 1.82s\n",
      "961:\tlearn: 0.0984235\ttotal: 44.9s\tremaining: 1.77s\n",
      "962:\tlearn: 0.0983953\ttotal: 45s\tremaining: 1.73s\n",
      "963:\tlearn: 0.0983688\ttotal: 45s\tremaining: 1.68s\n",
      "964:\tlearn: 0.0983175\ttotal: 45s\tremaining: 1.63s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965:\tlearn: 0.0982625\ttotal: 45.1s\tremaining: 1.59s\n",
      "966:\tlearn: 0.0982329\ttotal: 45.1s\tremaining: 1.54s\n",
      "967:\tlearn: 0.0981935\ttotal: 45.2s\tremaining: 1.49s\n",
      "968:\tlearn: 0.0981683\ttotal: 45.2s\tremaining: 1.45s\n",
      "969:\tlearn: 0.0981336\ttotal: 45.3s\tremaining: 1.4s\n",
      "970:\tlearn: 0.0981104\ttotal: 45.3s\tremaining: 1.35s\n",
      "971:\tlearn: 0.0980919\ttotal: 45.4s\tremaining: 1.31s\n",
      "972:\tlearn: 0.0980589\ttotal: 45.4s\tremaining: 1.26s\n",
      "973:\tlearn: 0.0980339\ttotal: 45.5s\tremaining: 1.21s\n",
      "974:\tlearn: 0.0980139\ttotal: 45.5s\tremaining: 1.17s\n",
      "975:\tlearn: 0.0979870\ttotal: 45.6s\tremaining: 1.12s\n",
      "976:\tlearn: 0.0979632\ttotal: 45.6s\tremaining: 1.07s\n",
      "977:\tlearn: 0.0979116\ttotal: 45.7s\tremaining: 1.03s\n",
      "978:\tlearn: 0.0978919\ttotal: 45.8s\tremaining: 982ms\n",
      "979:\tlearn: 0.0978550\ttotal: 45.8s\tremaining: 935ms\n",
      "980:\tlearn: 0.0978345\ttotal: 45.9s\tremaining: 888ms\n",
      "981:\tlearn: 0.0978021\ttotal: 45.9s\tremaining: 841ms\n",
      "982:\tlearn: 0.0977682\ttotal: 46s\tremaining: 795ms\n",
      "983:\tlearn: 0.0977204\ttotal: 46s\tremaining: 748ms\n",
      "984:\tlearn: 0.0976759\ttotal: 46.1s\tremaining: 701ms\n",
      "985:\tlearn: 0.0976469\ttotal: 46.1s\tremaining: 655ms\n",
      "986:\tlearn: 0.0976219\ttotal: 46.1s\tremaining: 608ms\n",
      "987:\tlearn: 0.0975812\ttotal: 46.2s\tremaining: 561ms\n",
      "988:\tlearn: 0.0975494\ttotal: 46.2s\tremaining: 514ms\n",
      "989:\tlearn: 0.0975180\ttotal: 46.3s\tremaining: 468ms\n",
      "990:\tlearn: 0.0974892\ttotal: 46.3s\tremaining: 421ms\n",
      "991:\tlearn: 0.0974452\ttotal: 46.4s\tremaining: 374ms\n",
      "992:\tlearn: 0.0974072\ttotal: 46.4s\tremaining: 327ms\n",
      "993:\tlearn: 0.0973879\ttotal: 46.5s\tremaining: 281ms\n",
      "994:\tlearn: 0.0973544\ttotal: 46.5s\tremaining: 234ms\n",
      "995:\tlearn: 0.0973220\ttotal: 46.6s\tremaining: 187ms\n",
      "996:\tlearn: 0.0972889\ttotal: 46.6s\tremaining: 140ms\n",
      "997:\tlearn: 0.0972583\ttotal: 46.7s\tremaining: 93.5ms\n",
      "998:\tlearn: 0.0972158\ttotal: 46.7s\tremaining: 46.8ms\n",
      "999:\tlearn: 0.0971934\ttotal: 46.8s\tremaining: 0us\n",
      "Target: yaw_error_mean\n",
      "Training MAE: 0.06545628873468469\n",
      "Training MSE: 0.009446555897138744\n",
      "Training RMSE: 0.09719339430814598\n",
      "Training R2: 0.8849169428992021\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "def add_squared_features(df):\n",
    "    squared_features = df.drop(target, axis=1).apply(lambda x: x**2)\n",
    "    squared_columns = [col + '_squared' for col in squared_features.columns]\n",
    "    df[squared_columns] = squared_features\n",
    "    return df\n",
    "\n",
    "def add_exponential_features(df):\n",
    "    exponential_features = df.drop(target, axis=1).apply(np.exp)\n",
    "    exponential_columns = [col + '_exp' for col in exponential_features.columns]\n",
    "    df[exponential_columns] = exponential_features\n",
    "    return df\n",
    "\n",
    "normalized_train_df_with_features = add_squared_features(normalized_train_df.copy())\n",
    "\n",
    "normalized_train_df_with_features = add_exponential_features(normalized_train_df_with_features)\n",
    "\n",
    "X_train = normalized_train_df_with_features.drop(target, axis=1)\n",
    "y_train = normalized_train_df_with_features[target]\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = catboost_model.predict(X_train)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_train, train_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Training MAE:\", train_mae)\n",
    "print(\"Training MSE:\", train_mse)\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Training R2:\", train_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "def add_squared_features(df):\n",
    "    squared_features = df.drop(target, axis=1).apply(lambda x: x**2)\n",
    "    squared_columns = [col + '_squared' for col in squared_features.columns]\n",
    "    df[squared_columns] = squared_features\n",
    "    return df\n",
    "\n",
    "normalized_train_df_with_features = add_squared_features(normalized_train_df.copy())\n",
    "\n",
    "X_train = normalized_train_df_with_features.drop(target, axis=1)\n",
    "y_train = normalized_train_df_with_features[target]\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = catboost_model.predict(X_train)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_train, train_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Training MAE:\", train_mae)\n",
    "print(\"Training MSE:\", train_mse)\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Training R2:\", train_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925aadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "def add_squared_features(df):\n",
    "    squared_features = df.drop(target, axis=1).apply(lambda x: x**2)\n",
    "    squared_columns = [col + '_squared' for col in squared_features.columns]\n",
    "    df[squared_columns] = squared_features\n",
    "    return df\n",
    "\n",
    "normalized_train_df_with_features = add_squared_features(normalized_train_df.copy())\n",
    "\n",
    "normalized_test_df_with_features = add_squared_features(normalized_test_df.copy())\n",
    "\n",
    "X_train = normalized_train_df_with_features.drop(target, axis=1)\n",
    "y_train = normalized_train_df_with_features[target]\n",
    "\n",
    "X_test = normalized_test_df_with_features.drop(target, axis=1)\n",
    "y_test = normalized_test_df_with_features[target]\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = catboost_model.predict(X_train)\n",
    "\n",
    "test_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_train, train_predictions)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Training MAE:\", train_mae)\n",
    "print(\"Training MSE:\", train_mse)\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Training R2:\", train_r2)\n",
    "print()\n",
    "print(\"Testing MAE:\", test_mae)\n",
    "print(\"Testing MSE:\", test_mse)\n",
    "print(\"Testing RMSE:\", test_rmse)\n",
    "print(\"Testing R2:\", test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f187bdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "def add_exponential_features(df):\n",
    "    exponential_features = df.drop(target, axis=1).apply(np.exp)\n",
    "    exponential_columns = [col + '_exp' for col in exponential_features.columns]\n",
    "    df[exponential_columns] = exponential_features\n",
    "    return df\n",
    "\n",
    "normalized_train_df_with_features = add_exponential_features(normalized_train_df.copy())\n",
    "\n",
    "normalized_test_df_with_features = add_exponential_features(normalized_test_df.copy())\n",
    "\n",
    "X_train = normalized_train_df_with_features.drop(target, axis=1)\n",
    "y_train = normalized_train_df_with_features[target]\n",
    "\n",
    "X_test = normalized_test_df_with_features.drop(target, axis=1)\n",
    "y_test = normalized_test_df_with_features[target]\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Test MAE:\", test_mae)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e1c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349b6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736660a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc70d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08de765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fed0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e5d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "def add_logarithmic_features(df):\n",
    "    logarithmic_features = df.drop(target, axis=1).apply(np.log)\n",
    "    logarithmic_columns = [col + '_log' for col in logarithmic_features.columns]\n",
    "    df[logarithmic_columns] = logarithmic_features\n",
    "    return df\n",
    "\n",
    "normalized_train_df_with_features = add_logarithmic_features(normalized_train_df.copy())\n",
    "\n",
    "normalized_test_df_with_features = add_logarithmic_features(normalized_test_df.copy())\n",
    "\n",
    "X_train = normalized_train_df_with_features.drop(target, axis=1)\n",
    "y_train = normalized_train_df_with_features[target]\n",
    "\n",
    "X_test = normalized_test_df_with_features.drop(target, axis=1)\n",
    "y_test = normalized_test_df_with_features[target]\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Test MAE:\", test_mae)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40689c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "X_train = normalized_train_df.drop(target, axis=1)\n",
    "y_train = normalized_train_df[target]\n",
    "\n",
    "X_test = normalized_test_df.drop(target, axis=1)\n",
    "y_test = normalized_test_df[target]\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = catboost_model.predict(X_train)\n",
    "test_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_train, train_predictions)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Training MAE:\", train_mae)\n",
    "print(\"Training MSE:\", train_mse)\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Training R2:\", train_r2)\n",
    "print()\n",
    "print(\"Test MAE:\", test_mae)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbdcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "def add_binned_features(df, num_bins=10):\n",
    "    binned_features = df.drop(target, axis=1).apply(lambda x: pd.cut(x, bins=num_bins, labels=False))\n",
    "    binned_columns = [col + '_binned' for col in binned_features.columns]\n",
    "    df[binned_columns] = binned_features\n",
    "    return df\n",
    "\n",
    "normalized_train_df_with_features = add_binned_features(normalized_train_df.copy())\n",
    "\n",
    "normalized_test_df_with_features = add_binned_features(normalized_test_df.copy())\n",
    "\n",
    "X_train = normalized_train_df_with_features.drop(target, axis=1)\n",
    "y_train = normalized_train_df_with_features[target]\n",
    "\n",
    "X_test = normalized_test_df_with_features.drop(target, axis=1)\n",
    "y_test = normalized_test_df_with_features[target]\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Test MAE:\", test_mae)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4cb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab41766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "X_train = normalized_train_df.drop(target, axis=1)\n",
    "y_train = normalized_train_df[target]\n",
    "\n",
    "X_test = normalized_test_df.drop(target, axis=1)\n",
    "y_test = normalized_test_df[target]\n",
    "\n",
    "hyperparameters = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'l2_leaf_reg': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(catboost_model, hyperparameters, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_catboost_model = CatBoostRegressor(**best_params)\n",
    "best_catboost_model.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = best_catboost_model.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Test MAE:\", test_mae)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6c71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8796de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Ensemble Methods\n",
    "random_forest = RandomForestRegressor()\n",
    "gradient_boosting = GradientBoostingRegressor()\n",
    "\n",
    "# Deep Learning Model\n",
    "mlp = MLPRegressor()\n",
    "\n",
    "X_train = normalized_train_df.drop(target, axis=1)\n",
    "y_train = normalized_train_df[target]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "ensemble_predictions = []\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    random_forest.fit(X_train_fold, y_train_fold)\n",
    "    gradient_boosting.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    rf_predictions = random_forest.predict(X_val_fold)\n",
    "    gb_predictions = gradient_boosting.predict(X_val_fold)\n",
    "    \n",
    "    ensemble_predictions.extend(list(zip(rf_predictions, gb_predictions)))\n",
    "\n",
    "stacking_X_train = pd.DataFrame(ensemble_predictions, columns=[\"RF_Predictions\", \"GB_Predictions\"])\n",
    "stacking_y_train = y_train.values\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacking_X_train, stacking_y_train)\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "X_test = normalized_test_df.drop(target, axis=1)\n",
    "y_test = normalized_test_df[target]\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "ensemble_test_predictions = []\n",
    "rf_test_predictions = random_forest.predict(X_test_scaled)\n",
    "gb_test_predictions = gradient_boosting.predict(X_test_scaled)\n",
    "ensemble_test_predictions.extend(list(zip(rf_test_predictions, gb_test_predictions)))\n",
    "\n",
    "stacking_X_test = pd.DataFrame(ensemble_test_predictions, columns=[\"RF_Predictions\", \"GB_Predictions\"])\n",
    "stacking_y_test = y_test.values\n",
    "\n",
    "stacking_predictions = meta_model.predict(stacking_X_test)\n",
    "mlp_predictions = mlp.predict(X_test_scaled)\n",
    "\n",
    "ensemble_mae = mean_absolute_error(y_test, np.mean(ensemble_test_predictions, axis=1))\n",
    "ensemble_mse = mean_squared_error(y_test, np.mean(ensemble_test_predictions, axis=1))\n",
    "ensemble_rmse = np.sqrt(ensemble_mse)\n",
    "ensemble_r2 = r2_score(y_test, np.mean(ensemble_test_predictions, axis=1))\n",
    "\n",
    "stacking_mae = mean_absolute_error(y_test, stacking_predictions)\n",
    "stacking_mse = mean_squared_error(y_test, stacking_predictions)\n",
    "stacking_rmse = np.sqrt(stacking_mse)\n",
    "stacking_r2 = r2_score(y_test, stacking_predictions)\n",
    "\n",
    "mlp_mae = mean_absolute_error(y_test, mlp_predictions)\n",
    "mlp_mse = mean_squared_error(y_test, mlp_predictions)\n",
    "mlp_rmse = np.sqrt(mlp_mse)\n",
    "mlp_r2 = r2_score(y_test, mlp_predictions)\n",
    "\n",
    "print(\"Ensemble MAE:\", ensemble_mae)\n",
    "print(\"Ensemble MSE:\", ensemble_mse)\n",
    "print(\"Ensemble RMSE:\", ensemble_rmse)\n",
    "print(\"Ensemble R2:\", ensemble_r2)\n",
    "print()\n",
    "print(\"Stacking MAE:\", stacking_mae)\n",
    "print(\"Stacking MSE:\", stacking_mse)\n",
    "print(\"Stacking RMSE:\", stacking_rmse)\n",
    "print(\"Stacking R2:\", stacking_r2)\n",
    "print()\n",
    "print(\"MLP MAE:\", mlp_mae)\n",
    "print(\"MLP MSE:\", mlp_mse)\n",
    "print(\"MLP RMSE:\", mlp_rmse)\n",
    "print(\"MLP R2:\", mlp_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4a49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "random_forest = RandomForestRegressor()\n",
    "gradient_boosting = GradientBoostingRegressor()\n",
    "\n",
    "X_train = normalized_train_df.drop(target, axis=1)\n",
    "y_train = normalized_train_df[target]\n",
    "\n",
    "X_test = normalized_test_df.drop(target, axis=1)\n",
    "y_test = normalized_test_df[target]\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "rf_predictions = random_forest.predict(X_test)\n",
    "\n",
    "gradient_boosting.fit(X_train, y_train)\n",
    "gb_predictions = gradient_boosting.predict(X_test)\n",
    "\n",
    "ensemble_predictions = (rf_predictions + gb_predictions) / 2\n",
    "\n",
    "ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)\n",
    "ensemble_mse = mean_squared_error(y_test, ensemble_predictions)\n",
    "ensemble_rmse = np.sqrt(ensemble_mse)\n",
    "ensemble_r2 = r2_score(y_test, ensemble_predictions)\n",
    "\n",
    "print(\"Ensemble MAE:\", ensemble_mae)\n",
    "print(\"Ensemble MSE:\", ensemble_mse)\n",
    "print(\"Ensemble RMSE:\", ensemble_rmse)\n",
    "print(\"Ensemble R2:\", ensemble_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19707c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a243b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc44323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.scatter(y_train, train_predictions)\n",
    "plt.xlabel('Actual Yaw Error (Training)')\n",
    "plt.ylabel('Predicted Yaw Error (Training)')\n",
    "plt.title('Non-linear Relationship: Actual vs. Predicted Yaw Error (Training)')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_test, test_predictions)\n",
    "plt.xlabel('Actual Yaw Error (Testing)')\n",
    "plt.ylabel('Predicted Yaw Error (Testing)')\n",
    "plt.title('Non-linear Relationship: Actual vs. Predicted Yaw Error (Testing)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c67360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b750c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "X_train = normalized_train_df.drop(target, axis=1)\n",
    "y_train = normalized_train_df[target]\n",
    "\n",
    "X_test = normalized_test_df.drop(target, axis=1)\n",
    "y_test = normalized_test_df[target]\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(catboost_model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert negative MSE scores to positive RMSE scores\n",
    "cv_rmse_scores = np.sqrt(-cv_scores)\n",
    "\n",
    "# Fit the model on the entire training data\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the testing data\n",
    "test_predictions = catboost_model.predict(X_test)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, catboost_model.predict(X_train))\n",
    "train_mse = mean_squared_error(y_train, catboost_model.predict(X_train))\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_train, catboost_model.predict(X_train))\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Training MAE:\", train_mae)\n",
    "print(\"Training MSE:\", train_mse)\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Training R2:\", train_r2)\n",
    "print()\n",
    "print(\"Test MAE:\", test_mae)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\", test_r2)\n",
    "print()\n",
    "print(\"Cross-Validation RMSE scores:\", cv_rmse_scores)\n",
    "print(\"Mean CV RMSE:\", np.mean(cv_rmse_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39af380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842e8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ff458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "X_train = normalized_train_df.drop(target, axis=1)\n",
    "y_train = normalized_train_df[target]\n",
    "\n",
    "X_test = normalized_test_df.drop(target, axis=1)\n",
    "y_test = normalized_test_df[target]\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'l2_leaf_reg': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(catboost_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "best_catboost_model = CatBoostRegressor(**best_params)\n",
    "best_catboost_model.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = best_catboost_model.predict(X_train)\n",
    "\n",
    "test_predictions = best_catboost_model.predict(X_test)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_train, train_predictions)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Training MAE:\", train_mae)\n",
    "print(\"Training MSE:\", train_mse)\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Training R2:\", train_r2)\n",
    "print()\n",
    "print(\"Test MAE:\", test_mae)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d9c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b7bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "X_train = normalized_train_df.drop(target, axis=1)\n",
    "y_train = normalized_train_df[target]\n",
    "\n",
    "X_test = normalized_test_df.drop(target, axis=1)\n",
    "y_test = normalized_test_df[target]\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(catboost_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_learning_rate = grid_search.best_params_['learning_rate']\n",
    "\n",
    "best_catboost_model = CatBoostRegressor(learning_rate=best_learning_rate)\n",
    "best_catboost_model.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = best_catboost_model.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Best Learning Rate:\", best_learning_rate)\n",
    "print(\"Test MAE:\", test_mae)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ac013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ef7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "target = 'yaw_error_mean'\n",
    "\n",
    "X_train = normalized_train_df.drop(target, axis=1)\n",
    "y_train = normalized_train_df[target]\n",
    "\n",
    "X_test = normalized_test_df.drop(target, axis=1)\n",
    "y_test = normalized_test_df[target]\n",
    "\n",
    "param_grid = {\n",
    "    'l2_leaf_reg': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(catboost_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_l2_leaf_reg = grid_search.best_params_['l2_leaf_reg']\n",
    "\n",
    "best_catboost_model = CatBoostRegressor(l2_leaf_reg=best_l2_leaf_reg)\n",
    "best_catboost_model.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = best_catboost_model.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Target:\", target)\n",
    "print(\"Best l2_leaf_reg:\", best_l2_leaf_reg)\n",
    "print(\"Test MAE:\", test_mae)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e16fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2ecc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d282cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8eadb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d65db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "X_train = normalized_train_df.drop('yaw_error_mean', axis=1)\n",
    "y_train = normalized_train_df['yaw_error_mean']\n",
    "\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "X_test = normalized_test_df.drop('yaw_error_mean', axis=1)\n",
    "y_pred = decision_tree.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Yaw Error')\n",
    "plt.ylabel('Predicted Yaw Error')\n",
    "plt.title('Non-linear Relationship: Actual vs. Predicted Yaw Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782db290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3363249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X_train = normalized_train_df.drop('yaw_error_mean', axis=1)\n",
    "y_train = normalized_train_df['yaw_error_mean']\n",
    "\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "X_test = normalized_test_df.drop('yaw_error_mean', axis=1)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R^2):\", r2)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Yaw Error')\n",
    "plt.ylabel('Predicted Yaw Error')\n",
    "plt.title('Non-linear Relationship: Actual vs. Predicted Yaw Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6e061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e1aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X_train = normalized_train_df.drop('yaw_error_mean', axis=1)\n",
    "y_train = normalized_train_df['yaw_error_mean']\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "X_test = normalized_test_df.drop('yaw_error_mean', axis=1)\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R^2):\", r2)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Yaw Error')\n",
    "plt.ylabel('Predicted Yaw Error')\n",
    "plt.title('Non-linear Relationship: Actual vs. Predicted Yaw Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1826b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4409bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f97b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38baae89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a76599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
